<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://gaotianyu.xyz/feed.xml" rel="self" type="application/atom+xml" /><link href="https://gaotianyu.xyz/" rel="alternate" type="text/html" /><updated>2024-09-03T11:21:18-04:00</updated><id>https://gaotianyu.xyz/feed.xml</id><entry><title type="html">Teach Llamas to Talk: Recent Progress in Instruction Tuning</title><link href="https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning/" rel="alternate" type="text/html" title="Teach Llamas to Talk: Recent Progress in Instruction Tuning" /><published>2023-11-30T00:00:00-05:00</published><updated>2023-11-30T00:00:00-05:00</updated><id>https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning</id><content type="html" xml:base="https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning/">&lt;p&gt;&lt;em&gt;Disclaimer: this is not a comprehensive review of instruction-tuning or RLHF literature, but a brief introduction of the recent progress (and a little promotion on our work). However, any comments/suggestions are welcome and please feel free to email me (tianyug@princeton.edu) and I would love to have a discussion!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Large language models (LLMs), powered by billions of parameters and trained with trillions of tokens, are quite powerful as they can handle a variety of tasks out of the box. However, to be useful in real-world applications and to act as a general task-solving machine, they must master following user instructions and responding in a coherent and helpful way, instead of being a mere “stochastic parrot”, echoing the chaotic language patterns from the Internet. Thus, open-ended instruction tuning&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; (&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Ouyang et al., 2022&lt;/a&gt;; InstructGPT), which fine-tunes an LLM such that it can follow user instructions and respond in a helpful, honest, and harmless way (Anthropic’s HHH criteria; &lt;a href=&quot;https://arxiv.org/abs/2112.00861&quot;&gt;Askell et al., 2021&lt;/a&gt;), emerged as a promising approach. The interest further increased after the huge success of ChatGPT. Open-ended instruction tuning usually contains two stages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Supervised fine-tuning&lt;/strong&gt; (SFT) the model on collected user &lt;em&gt;instructions&lt;/em&gt; and gold &lt;em&gt;responses.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Aligning&lt;/strong&gt; (the primary method for this is reinforcement learning from human feedback; RLHF; &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Ouyang et al., 2022&lt;/a&gt;) the model with human preferences. This usually requires human preference data, which comprise response pairs and annotations of which one is better.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Collecting supervised fine-tuning or preference data is known to be prohibitively expensive (&lt;a href=&quot;https://www.interconnects.ai/p/llama-2-from-meta&quot;&gt;Lambert, 2023&lt;/a&gt;), thus it stayed as a corporate game until 2023, when people found cheaper ways to construct such data. Since then there have been numerous open-source efforts in developing instruction-tuned models. In the following, I will cover such efforts in four parts: &lt;strong&gt;SFT data&lt;/strong&gt;, &lt;strong&gt;preference data&lt;/strong&gt;, &lt;strong&gt;algorithms&lt;/strong&gt;, and &lt;strong&gt;evaluation&lt;/strong&gt;. In the end, I will introduce &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.07641&quot;&gt;our latest work&lt;/a&gt;&lt;/strong&gt; on instruction-following evaluation, which shows that it is important to set up the right evaluator and otherwise you may get misleading results.&lt;/p&gt;

&lt;h2 id=&quot;supervised-fine-tuning-sft-data&quot;&gt;Supervised fine-tuning (SFT) data&lt;/h2&gt;

&lt;p&gt;There are, in general, two purposes of supervised fine-tuning and they correspond to two types of data. One is to further improve general language understanding abilities of LLMs, reflected in traditional NLP benchmarks like HellaSwag (&lt;a href=&quot;https://arxiv.org/abs/1905.07830&quot;&gt;Zellers et al., 2019&lt;/a&gt;), MMLU (&lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;&gt;Hendrycks et al., 2021&lt;/a&gt;), etc. The other is to train LLMs to follow instructions, acquire conversational abilities, and be helpful and harmless (&lt;a href=&quot;https://arxiv.org/abs/2112.00861&quot;&gt;Askell et al., 2021&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Corresponding to the first purpose, there is &lt;em&gt;multi-task instruction tuning&lt;/em&gt; data, which have been heavily explored between 2020-2022. Those data combine thousands of NLP tasks together and give each task a natural language instruction, then one can train models on the combination in a multi-task way (for a more thorough review, see Sebastian Ruder’s great &lt;a href=&quot;https://nlpnewsletter.substack.com/p/instruction-tuning-vol-1?utm_source=profile&amp;amp;utm_medium=reader2&quot;&gt;blogpost&lt;/a&gt;). Representative datasets include Natural Instruction (&lt;a href=&quot;https://arxiv.org/abs/2104.08773&quot;&gt;Mishra et al., 2021&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2204.07705&quot;&gt;Wang et al., 2022&lt;/a&gt;), T0 (&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;&gt;San et al., 2021&lt;/a&gt;), and Flan (&lt;a href=&quot;https://arxiv.org/abs/2109.01652&quot;&gt;Wei et al., 2021&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;&gt;Chung et al., 2022&lt;/a&gt;). Different from open-ended instruction tuning, those datasets/models target traditional NLP tasks (question answering, natural language inference, etc.) more and tend to have shorter/simpler/less diverse instructions and responses — imagine the difference between “classify the sentiment of the sentence” and “write me a personal webpage with a similar style as OpenAI’s blog by using Jekyll.” Therefore, models trained on these datasets usually are not deployed as nowadays “instruction-tuned” models or chatbots, despite their strong performance on NLP benchmarks. &lt;a href=&quot;https://arxiv.org/abs/2306.04751&quot;&gt;Wang et al., 2023&lt;/a&gt; (TÜLU) showed that combining these datasets with the new open-ended instruction tuning datasets can improve both the general language understanding ability and the instruction following ability. &lt;a href=&quot;https://arxiv.org/abs/2306.02707&quot;&gt;Mukherjee et al., 2023&lt;/a&gt; (Orca) found that using those data as seeds, prompting GPT-4 to output answers with explanations, and imitating GPT-4’s responses can significantly improve weaker model’s performance. The mixture usage of data is adopted in some of the public instruction-tuned models, such as &lt;a href=&quot;https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models&quot;&gt;Stable Beluga&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now let’s talk about open-ended instruction tuning data, which notably emerged in 2023 (in the following, “SFT data” refer to open-ended instruction tuning data). It is general belief that training with these data does not improve LLMs’ “knowledge” (reflected by scores on traditional benchmarks), but merely “guides” them to follow the instruction-following or conversational format, gaining an engaging tone, being polite, etc (superficial alignment hypothesis; &lt;a href=&quot;https://arxiv.org/abs/2305.11206&quot;&gt;Zhou et al., 2023&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Collecting SFT data is expensive, as one needs to both collect user instructions and annotate demonstration responses (&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Ouyang et al., 2022&lt;/a&gt;). One primary way for open-source models to get open-ended instruction tuning data is to distill from proprietary LLMs. One of the earliest open-source instruction models, &lt;a href=&quot;https://crfm.stanford.edu/2023/03/13/alpaca.html&quot;&gt;Alpaca&lt;/a&gt;, used self-instruct (&lt;a href=&quot;https://arxiv.org/pdf/2212.10560.pdf&quot;&gt;Wang et al., 2023&lt;/a&gt;) to prompt &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt; (a version of InstructGPT model) and generate pseudo SFT data, and then SFTed LLaMA-7B on it; &lt;a href=&quot;https://github.com/project-baize/baize-chatbot&quot;&gt;Baize&lt;/a&gt; (&lt;a href=&quot;https://github.com/project-baize/baize-chatbot&quot;&gt;Xu et al., 2023&lt;/a&gt;) also used self-instruct, but instead prompted ChatGPT to self-chat for acquiring multi-turn data; WizardLM (&lt;a href=&quot;https://arxiv.org/abs/2304.12244&quot;&gt;Xu et al., 2023&lt;/a&gt;) improved data diversity by using ChatGPT to rewrite Alpaca data iteratively; UltraChat (&lt;a href=&quot;https://arxiv.org/abs/2305.14233&quot;&gt;Ding et al., 2023&lt;/a&gt;) first constructed questions automatically using different strategies, then prompted ChatGPT to simulate a conversation given the question. &lt;a href=&quot;https://lmsys.org/blog/2023-03-30-vicuna/&quot;&gt;Vicuna&lt;/a&gt; and &lt;a href=&quot;https://bair.berkeley.edu/blog/2023/04/03/koala/&quot;&gt;Koala&lt;/a&gt; explored &lt;a href=&quot;https://sharegpt.com&quot;&gt;SharGPT&lt;/a&gt;, a website where users share their chats with ChatGPT, as its SFT data. A recent similar effort, &lt;a href=&quot;https://openreview.net/pdf?id=Bl8u7ZRlbM&quot;&gt;WildChat&lt;/a&gt;, provided online users free ChatGPT access and collected the conversations, though the focus was more on studying toxic use cases. Even though it is a relative cheap way to acquire data, imitating proprietary LLMs is found to just “mimic ChatGPT’s style but not its factuality” (&lt;a href=&quot;https://arxiv.org/abs/2305.15717&quot;&gt;Gudibande et al., 2023&lt;/a&gt;), hence putting a question on how far open-source models can go solely relying on such SFT data.&lt;/p&gt;

&lt;p&gt;Another way to collect SFT data is to manually annotate a small amount of data. &lt;a href=&quot;https://open-assistant.io&quot;&gt;Open Assistant&lt;/a&gt; initiated a crowd-sourcing effort where volunteers write both instructions and responses; &lt;a href=&quot;https://github.com/databrickslabs/dolly&quot;&gt;Dolly&lt;/a&gt; contains 15k Databricks-employee-generated data (more towards Wikipedia-based factoid QA). LIMA (”less is more for alignment”; &lt;a href=&quot;https://arxiv.org/abs/2305.11206&quot;&gt;Zhou et al., 2023&lt;/a&gt;), a collection of author-curated 1,000 SFT data (have a distribution heavily steered towards Stack Exchange and wikiHow), is found to be surprisingly effective in producing strong instruction models. However, whether we only need 1,000 examples&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; or we can use Internet-crowdsourced data, comparing to using dedicatedly-collected large-scale data, remains a question, as there is not apple-to-apple comparison.&lt;/p&gt;

&lt;p&gt;While open-source models trained on these imitation and human SFT data are still not comparable to proprietary models like ChatGPT, GPT-4, or Claude (&lt;a href=&quot;https://chat.lmsys.org&quot;&gt;Chatbot Arena&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2306.05685&quot;&gt;Zheng et al., 2023&lt;/a&gt;), we see two promises:&lt;/p&gt;

&lt;p&gt;(1) LLaMA-2-70B-chat, a LLaMA-2-70B (open-source) model tuned on &lt;strong&gt;closed-source&lt;/strong&gt; data, is shown to be “more helpful” than ChatGPT by human evaluation (&lt;a href=&quot;https://arxiv.org/abs/2307.09288&quot;&gt;Touvron et al., 2023&lt;/a&gt;). This shows that we have a base model (LLaMA-2) that is potentially as strong as ChatGPT’s base model (in terms of factual knowledge, commonsense, reasoning ability, etc.), mitigating the “false promise” problem (&lt;a href=&quot;https://arxiv.org/abs/2305.15717&quot;&gt;Gudibande et al., 2023&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;(2) Our research community has already conducted exciting research on the “toy” or “laboratory” data, such as better alignment algorithms, which will be mentioned below.&lt;/p&gt;

&lt;h2 id=&quot;what-about-preference-data&quot;&gt;What about preference data?&lt;/h2&gt;

&lt;p&gt;Despite the impressive “illusion” that open-source SFT models give us (in fact, they ignited the trend of open-source effort in instruction tuning), merely having SFT is not enough. Aligning the model with human preference data is essential for models to be a better language assistant. An easy way to reason about it is to think about how to “be honest”: SFT almost always encourages the model to give an answer and hardly teaches the model to say “I don’t know about this” (Yoav Goldberg’s &lt;a href=&quot;https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81&quot;&gt;blogpost&lt;/a&gt;; John Schulman’s &lt;a href=&quot;https://www.youtube.com/watch?v=hhiLw5Q_UFg&quot;&gt;Berkeley talk&lt;/a&gt;). Alignment algorithms have also been shown to bring better “human satisfaction” in several works (&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Ouyang et al., 2022&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2204.05862&quot;&gt;Bai et al., 2022&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2307.09288&quot;&gt;Touvron et al., 2023&lt;/a&gt;). However, most open-source models have not gone through the alignment stage (RLHF), due to (1) the high cost to run RL, (2) the brittleness to tune PPO (the RL algorithm used by OpenAI) hyperparameters, and (3) the lack of high-quality preference data. The lack of data further hinders the community effort to (potentially) create better algorithms that are more efficient/effective than RL.&lt;/p&gt;

&lt;p&gt;The most commonly used two preference datasets for developing aligning algorithms are OpenAI’s TL;DR preference data (summarization; &lt;a href=&quot;https://arxiv.org/abs/2009.01325&quot;&gt;Stiennon et al., 2020&lt;/a&gt;) and Anthropic’s HH-RLHF dataset (human-model open-ended dialogues; &lt;a href=&quot;https://arxiv.org/abs/2204.05862&quot;&gt;Bai et al., 2022&lt;/a&gt;).&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Though they both have good qualities, the diversity and the complexity of instructions are not comparable to nowadays’ SFT data.&lt;/p&gt;

&lt;p&gt;In 2023, there emerged a number of new preference datasets. While they can be valuable resources to researchers, whether their qualities suffice for alignment algorithms remains to be seen. There are crowd-sourcing efforts to collect preferences from humans: Both &lt;a href=&quot;https://open-assistant.io&quot;&gt;Open Assistant&lt;/a&gt; and &lt;a href=&quot;https://lmsys.org/blog/2023-07-20-dataset/&quot;&gt;Chatbot Arena&lt;/a&gt; launched a preference data collection campaign on the Internet and collected preference labels from volunteers. More datasets take a simulated or heuristic approach: &lt;a href=&quot;https://huggingface.co/datasets/stanfordnlp/SHP&quot;&gt;SHP&lt;/a&gt; uses numbers-of-upvote heuristics on Reddit to construct a synthetic preference dataset; &lt;a href=&quot;https://crfm.stanford.edu/2023/05/22/alpaca-farm.html&quot;&gt;AlpacaFarm&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/abs/2305.14387&quot;&gt;Dubois et al., 2023&lt;/a&gt;) and &lt;a href=&quot;https://huggingface.co/datasets/openbmb/UltraFeedback&quot;&gt;UltraFeedback&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/abs/2310.01377&quot;&gt;Cui et al., 2023&lt;/a&gt;) use GPT-4 as a gold annotator; &lt;a href=&quot;https://arxiv.org/abs/2305.13735&quot;&gt;Kim et al., 2023&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2310.02263&quot;&gt;Xu et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/pdf/2307.12950.pdf&quot;&gt;Yang et al., 2023&lt;/a&gt; use heuristics such as outputs of stronger models should be preferred or outputs from a “good” prompt should be preferred. There is evidence showing that most of them (&lt;a href=&quot;https://crfm.stanford.edu/2023/05/22/alpaca-farm.html&quot;&gt;AlpacaFarm&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/datasets/openbmb/UltraFeedback&quot;&gt;UltraFeedback&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2305.13735&quot;&gt;Kim et al., 2023&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/2310.02263&quot;&gt;Xu et al., 2023&lt;/a&gt;) can help RL or other alignment algorithms, but there is no apple-to-apple comparison. Huggingface recently released a model (Zephyr; &lt;a href=&quot;https://arxiv.org/abs/2310.16944&quot;&gt;Tunstall et al., 2023&lt;/a&gt;) that is trained with &lt;a href=&quot;https://arxiv.org/abs/2305.14233&quot;&gt;UltraChat&lt;/a&gt; (SFT) and &lt;a href=&quot;https://huggingface.co/datasets/openbmb/UltraFeedback&quot;&gt;UltraFeedback&lt;/a&gt; (alignment, using DPO), which is shown to have comparable performance to LLaMA-2-Chat-70B, a model trained on closed-source data.&lt;/p&gt;

&lt;p&gt;In contrary to relying on human preferences, another line of work tries to use “AI feedback” — using LLMs to guide LLMs without human involvement. The idea is different from “using GPT-4 as an annotator”, as GPT-4 is still trained with human preferences but here the goal is for the model to bootstrap without human preference data. &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;Bai et al., 2022&lt;/a&gt; first proposed two concepts: “constitutional AI”, which defines a series of “principles” that good generations should follow and prompts an SFT model to self-improve its generations (by self-critiques and revision), and then fine-tunes the model on the improved generations; and “reinforcement learning from AI feedback (RLAIF)”, where one prompts the SFT model to generate preferences over output pairs (instead of human)&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. Experiment wise, they demonstrate that starting from a model trained only with “helpfulness” human supervision, it is possible to train the model to be “harmless” (no human supervision on harmlessness). However, the pipeline in &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;Bai et al., 2022&lt;/a&gt; still starts from some human preference labels. &lt;a href=&quot;https://arxiv.org/abs/2309.00267&quot;&gt;Lee et al., 2023&lt;/a&gt; demonstrated that starting from an SFT model, RLAIF achieves comparable performance to RLHF on a summarization task, with no human preference label involved. The direction of RLAIF has generated considerable excitement and interest, as it is a viable solution to “scalable oversight” (&lt;a href=&quot;https://arxiv.org/abs/2211.03540&quot;&gt;Bowman et al., 2022&lt;/a&gt;), a scenario when the models-to-be-aligned have beyond-human capacities. However, how good these methods really are still remains unclear, as using simple heuristics to construct data (also no human involvement) can outperform them (RLCD; &lt;a href=&quot;https://arxiv.org/pdf/2307.12950.pdf&quot;&gt;Yang et al., 2023&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;is-rl-the-only-way&quot;&gt;Is RL the only way?&lt;/h2&gt;

&lt;p&gt;Using PPO (&lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;Schulman et al., 2017&lt;/a&gt;) for RLHF (the most famous ones: &lt;a href=&quot;https://arxiv.org/abs/2009.01325&quot;&gt;Stiennon et al., 2020&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Ouyang et al., 2022&lt;/a&gt;; though the idea of RLHF dates back to 2016 in the RL community) has been the primary methods for alignment (for example, it’s used for InstructGPT, believed to be used in ChatGPT and GPT-4, and is used for &lt;a href=&quot;https://arxiv.org/abs/2307.09288&quot;&gt;LLaMA-2-Chat&lt;/a&gt;). The basic idea is that you first train a reward model on preference data, then you can use the reward model to provide feedback and use RL to tune the model. There has been a rich body of literature on RLHF and you can read &lt;a href=&quot;https://huggingface.co/blog/rlhf&quot;&gt;this HuggingFace blogpost&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;RLHF is effective, but is complicated to implement, prone to optimization instability, and sensitive to hyperparameters (&lt;a href=&quot;https://arxiv.org/abs/2305.18290&quot;&gt;Rafailov et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2306.01693&quot;&gt;Wu et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2304.05302&quot;&gt;Yuan et al., 2023&lt;/a&gt;). Excitingly, there are a number of new methods proposed that can align models with preference data, and some of them are claimed to be stronger than RLHF.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Best-of-n.&lt;/strong&gt; The first intuition we can use is that models, after SFT, already have the potential to generate good outputs and we just need to pick them out. In WebGPT (&lt;a href=&quot;https://arxiv.org/abs/2112.09332&quot;&gt;Nakano et al., 2021&lt;/a&gt;) and summarization from human feedback (&lt;a href=&quot;https://arxiv.org/abs/2009.01325&quot;&gt;Stiennon et al., 2022&lt;/a&gt;), authors explored best-of-n sampling — sampling n outputs and using a reward model to pick the best — and showed that this often can achieve similar performance to RLHF. However, as pointed out by &lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;&gt;this OpenAI blogpost&lt;/a&gt;, best-of-n is inefficient if the final optimal policy is very far away from the original SFT model (n increases exponentially to the KL between final policy and the SFT model), not to mention that even if n is small, it is very inefficient for inference.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expert iteration.&lt;/strong&gt; Then there come methods that use best-of-n in training — we can sample a lot during training (no inference efficiency concerns), pick the best ones, and SFT on them. &lt;a href=&quot;https://platform.openai.com/docs/model-index-for-researchers&quot;&gt;FeedME&lt;/a&gt;&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, the method that is used to produce OpenAI’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-002&lt;/code&gt;, trains the model on its own sampled outputs that are liked by human labelers. One step further, this can be combined with sampling the best-of-n online (sample n outputs, pick the best one by a reward model, and then train on the best one, repeat), which is essentially expert iteration (&lt;a href=&quot;https://arxiv.org/abs/1705.08439&quot;&gt;Anthony et al., 2017&lt;/a&gt;); the sampling of best-of-n can also be combined with natural language feedback (&lt;a href=&quot;https://arxiv.org/abs/2303.16755&quot;&gt;Scheurer et al., 2023&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conditional tokens.&lt;/strong&gt; Another idea is “conditional tokens” (&lt;a href=&quot;https://arxiv.org/abs/2205.13636&quot;&gt;Lu et al., 2022&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2302.08582&quot;&gt;Korbak et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2302.02676&quot;&gt;Liu et al., 2023&lt;/a&gt;): one can do SFT on LMs with both good and bad examples, and prepend a “good” prompt to good examples and a “bad” prompt to bad examples. During inference, you can condition the model with the “good” prefix and expect the model to generate good outputs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contrastive-based methods.&lt;/strong&gt; Finally, there are several newly proposed methods that closely resemble the idea of contrastive learning: one can get probabilities of both good and bad examples from the model, and can “promote” the good ones while “repressing” the bad ones. Given preference data, SLiC (&lt;a href=&quot;https://arxiv.org/abs/2210.00045&quot;&gt;Zhao et al., 2022&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2305.10425&quot;&gt;2023&lt;/a&gt;) and RRHF (&lt;a href=&quot;https://arxiv.org/abs/2304.05302&quot;&gt;Yuan et al., 2023&lt;/a&gt;) optimize both a contrastive ranking loss and a regularization loss. For example, the SLiC loss function is as following,&lt;/p&gt;

&lt;p&gt;[[
\mathcal{L}(\theta)=max(0, \delta-\log \pi_\theta(y_w|x)+\log \pi_\theta (y_l|x))-\lambda \log \pi_\theta(y_{\text{ref}}|x),
]]&lt;/p&gt;

&lt;p&gt;where $\pi_\theta$ is the language model and $x,y_w,y_l,y_{\text{ref}}$ are the instruction, winning output, losing output, and the reference output. Intuitively, the first term enforces the contrast over the preferred/dispreferred pair and the second term makes sure the model does not deviates too much from the SFT distribution. Similarly, PRO (&lt;a href=&quot;https://arxiv.org/abs/2306.17492&quot;&gt;Song et al., 2023&lt;/a&gt;) adopts a Softmax-form of contrastive loss and optimizes over multiple negative outputs instead of just one.&lt;/p&gt;

&lt;p&gt;DPO (&lt;a href=&quot;https://arxiv.org/abs/2305.18290&quot;&gt;Rafailov et al., 2023&lt;/a&gt;) takes on a similar idea but starts from the RLHF’s objective function. After some theoretical analysis, DPO shows that optimizing the RLHF objective&lt;/p&gt;

&lt;div&gt;
[[
\mathbb{E}_{x\in D, y\in \pi_\theta(y|x)} \left[ r_\phi (x,y)\right] - \beta \text{D}_\text{KL} \left[\pi_\theta(y|x) || \pi_\text{ref} (y|x) \right],
]]
&lt;/div&gt;

&lt;p&gt;is equivalent to optimizing the following objective using MLE:&lt;/p&gt;

&lt;div&gt;
[[
-\mathbb{E}_{(x,y_w,y_l)\in D}\left[ \log \sigma \left( \beta \log \frac{\pi_\theta (y_w|x)}{\pi_\text{ref} (y_w|x)} -\beta \log \frac{\pi_\theta (y_l|x)}{\pi_\text{ref} (y_l|x)} \right) \right],
]]
&lt;/div&gt;

&lt;p&gt;where $r_\phi(x,y)$ is the reward model,  $\sigma(\cdot)$ is the sigmoid function, $\pi_\theta$ is the current model, and $\pi_\text{ref}$ is the SFT model.&lt;/p&gt;

&lt;p&gt;One downside of these models is that they either sample $y_w,y_l$ from the SFT model or take them directly from existing datasets (thus, sampled from other models), creating a distribution mismatch. &lt;a href=&quot;https://arxiv.org/pdf/2309.06657.pdf&quot;&gt;Liu et al., 2023&lt;/a&gt; (RSO) proposed to fix this problem with sampling from the optimal policy $\pi^*$ — by doing reject sampling with the reward model. They showed that applying such sampling strategy on top of SLiC or DPO can improve the final model performance.&lt;/p&gt;

&lt;p&gt;These methods have received much attention recently and are shown by other parties to be effective — for example, &lt;a href=&quot;https://arxiv.org/abs/2310.02263&quot;&gt;Xu et al., 2023&lt;/a&gt; demonstrated that DPO can bring significant improvement over SFT and HuggingFace’s Zephyr model (&lt;a href=&quot;https://arxiv.org/abs/2310.16944&quot;&gt;Tunstall et al., 2023&lt;/a&gt;), also trained with DPO, achieves strong performance on MT-Bench, even comparable to Llama-2-chat and GPT-3.5. As these methods are much cheaper than RL, it is good news to the research and open-source community and can potentially inspire more, better alignment algorithms.&lt;/p&gt;

&lt;p&gt;On the other hand, we need to better understand the properties of models trained with alignment algorithms and whether they truly help learn useful features — for example, &lt;a href=&quot;https://arxiv.org/abs/2310.03716&quot;&gt;Singhal et al., 2023&lt;/a&gt; showed that on several popular datasets, the learned reward models often have preferences highly correlated to length, and RLHF with length can recover most of the performance improvement.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;A notable challenge in developing open-ended instruction-tuned models (or any open-ended generation methods) is the evaluation. Human evaluation remains the “gold standard” for assessing the abilities of open-ended conversational models. However, human evaluation is often unreliable, especially if one uses cheap crowdsourcing platforms like Amazon Mechanical Turk (&lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.97.pdf&quot;&gt;Karpinska et al., 2021&lt;/a&gt;). Moreover, human evaluation is costly and hard to conduct apple-to-apple comparison with. Recently, people start to use stronger LLMs (e.g., ChatGPT or GPT-4) to evaluate weaker LLMs (e.g., open-source LLaMA-based models) — this is known as “LLM evaluators” — and it turns out to be a popular cost-effective alternative.&lt;/p&gt;

&lt;p&gt;At first sight, evaluating models with models sounds ridiculous. However, it makes sense for developing open-source and research models: proprietary models like GPT-4 are trained from a much stronger base model and are trained on instruction data with much higher quality and quantity, thus they are superior compared to open-source or research models. As long as there is such a big ability gap, models like GPT-4 should suffice as an evaluator.&lt;/p&gt;

&lt;p&gt;Several pilot works using LLMs as evaluators demonstrated “reassuring” results: LLM evaluators often have strong agreement with human evaluation (&lt;a href=&quot;https://arxiv.org/abs/2305.01937&quot;&gt;Chiang and Lee, 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2305.14387&quot;&gt;Dubois et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2304.00723&quot;&gt;Chen et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2306.05685&quot;&gt;Zheng et al., 2023&lt;/a&gt;). On the other hand, a number of papers showed that LLM evaluators are often extremely sensitive to certain biases. For example, they often change their preferences if you swap the two outputs to be compared (&lt;a href=&quot;https://arxiv.org/abs/2305.17926&quot;&gt;Wang et al., 2023&lt;/a&gt;). They also favor longer outputs and outputs generated by a similar model (&lt;a href=&quot;https://arxiv.org/abs/2306.05685&quot;&gt;Zheng et al., 2023&lt;/a&gt;). Therefore, there are several “meta-evaluation” benchmarks proposed to evaluate how good LLM evaluators are (usually in the form of accuracy on human preference data), namely FairEval (&lt;a href=&quot;https://arxiv.org/abs/2305.17926&quot;&gt;Wang et al., 2023&lt;/a&gt;), MT-Bench (&lt;a href=&quot;https://arxiv.org/abs/2306.05685&quot;&gt;Zheng et al., 2023&lt;/a&gt;), and LLMEval^2 (&lt;a href=&quot;https://arxiv.org/pdf/2308.01862.pdf&quot;&gt;Zhang et al., 2023&lt;/a&gt;). While these are valuable resources for us to understand how reliable LLM evaluators are, different evaluators often have close scores on these benchmarks. Moreover, the human annotations of these benchmarks are often noisy and subjective, and the intrinsic human agreement rate is quite low (e.g., &lt;a href=&quot;https://arxiv.org/abs/2305.14387&quot;&gt;AlpacaFarm&lt;/a&gt; reports a human agreement of 66%, &lt;a href=&quot;https://arxiv.org/abs/2306.05685&quot;&gt;MT-Bench&lt;/a&gt; reports a human agreement rate of 63%&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, and FairEval reports 71.7%). It is then unclear whether we can trust those meta-evaluation benchmarks, and the LLM evaluators.&lt;/p&gt;

&lt;h2 id=&quot;llmbar-a-better-meta-evaluation-of-llm-evaluators&quot;&gt;LLMBar: a better meta-evaluation of LLM evaluators&lt;/h2&gt;

&lt;p&gt;In our recent work, &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.07641&quot;&gt;Evaluating Large Language Models at Evaluating Instruction Following&lt;/a&gt;&lt;/strong&gt;, we rethink the problem of meta-evaluation. We argue that previous works ignore one important factor — the intrinsic subjectivity of human preferences.&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/2023-11-30-instruction-tuning/Untitled.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;90%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Given the above example from a previous dataset, even though the quality difference between the two is discernible, human annotators prefer the longer one, adding this bias to the preference dataset. When we assess LLM evaluators based on such subjective and noisy meta benchmarks, we cannot guarantee that the high-scoring evaluators can reliably evaluate objective properties, such as instruction following or factual correctness, over subjective preferences such as the output length.&lt;/p&gt;

&lt;p&gt;Following this path, we create a new meta-evaluation benchmark, LLMBar, that focuses on one objective criterion — &lt;strong&gt;instruction following&lt;/strong&gt;. We choose instruction following because (1) it is an ability that can be objectively evaluated; (2) it is directly related to desirable LLM properties such as &lt;em&gt;helpfulness&lt;/em&gt; (&lt;a href=&quot;https://arxiv.org/abs/2112.00861&quot;&gt;Askell et al., 2021&lt;/a&gt;); (3) unlike superficial qualities that can be easily acquired via imitation learning, even the strongest LLMs today struggle on this matter (&lt;a href=&quot;https://arxiv.org/abs/2307.02477&quot;&gt;Wu et al., 2023&lt;/a&gt;). One example from LLMBar is as follows,&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/2023-11-30-instruction-tuning/Untitled%201.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;90%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Even though it’s clear that the right output follows the instruction, both human and LLM evaluators often prefer the left one due to its more engaging tone. If we do not rigorously analyze the ability of evaluators to distinguishing between the true instruction following ability and superficial clues, there is a risk of advancing models that excel in mimicking conversational assistants rather than executing desired tasks.&lt;/p&gt;

&lt;p&gt;In LLMBar, the authors manually curated 419 instances, where each entry consists of an instruction paired with two outputs: one faithfully follows the instruction and the other deviates, and there always exists an objective preference. Thanks to the objective criterion and the manual curation, LLMBar has a human agreement rate of &lt;strong&gt;94%&lt;/strong&gt;. We test the evaluators on those output pairs and compare the evaluator preferences to our gold labels. We also curate an adversarial set, where the “bad” output often has some superficial appeal (length, engaging tones, generated by a better LM, etc.) that may mislead an evaluator. LLMBar demonstrates surprising results:&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/2023-11-30-instruction-tuning/Untitled%202.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;While ChatGPT, LLaMA2-70B-Chat, PaLM2-bison, and GPT-4 perform similarly on other meta-evaluation benchmarks, they demonstrate very distinct performance on LLMBar (adversarial) — ChatGPT and LLaMA2 score even below random guess, and GPT-4 is much more accurate than any other evaluator.&lt;/p&gt;

&lt;p&gt;Besides different LLMs, we also show that different prompts matter a lot for the evaluator. Several previous works explored in this direction: &lt;a href=&quot;https://arxiv.org/abs/2305.17926&quot;&gt;Wang et al., 2023&lt;/a&gt; proposed sampling multiple explanations and aggregating them into a final judgment; &lt;a href=&quot;https://arxiv.org/abs/2306.05685&quot;&gt;Zheng et al., 2023&lt;/a&gt; suggested a reference-guided method, where the LLM evaluator first generates its own output given the instruction, and then uses it as a reference; there are also several papers showing that deploying multiple evaluators (different LLMs or prompts) and letting them communicate or synthesize their judgements can improve the evaluator accuracy (&lt;a href=&quot;https://arxiv.org/abs/2307.02762&quot;&gt;Li et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/pdf/2308.01862.pdf&quot;&gt;Zhang et al., 2023&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/2308.07201&quot;&gt;Chan et al., 2023&lt;/a&gt;). In our work, we propose a combo of methods: &lt;strong&gt;metrics+reference+rules&lt;/strong&gt; (as shown below). We first prompt the LLM to generate three instruction-specific metrics or rubrics (a recent work, &lt;a href=&quot;https://arxiv.org/abs/2310.15123&quot;&gt;Saha et al., 2023&lt;/a&gt;, proposed a similar strategy); we also prompt the LLM to generate a reference output. Then, we feed the LLM the metrics and the reference, explicitly list the rules (e.g., focusing on instruction following, ignoring positional bias), and ask the model to give a judgement. Compared to a vanilla prompt used in &lt;a href=&quot;https://arxiv.org/abs/2305.14387&quot;&gt;AlpacaFarm&lt;/a&gt;, our new prompt significantly improves the evaluator performance on LLMBar (10% boost for GPT-4 on the adversarial set). We have more ablation studies in the paper and more interesting results, for example, chain of thought (&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;Wei et al., 2023&lt;/a&gt;) hurts the evaluator accuracy most of the time, a counter-intuitive finding.&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/2023-11-30-instruction-tuning/Untitled%203.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;90%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;looking-forward&quot;&gt;Looking forward&lt;/h2&gt;

&lt;p&gt;The emergence of open-source instruction-tuning data, algorithms, and models is one of the most exciting progress for LLMs in 2023. It gives researchers and developers the chance to train, evaluate, interact, and analyze instruction models with full control (from parameters to data), which only existed as black boxes before. The past few months are also a bit “chaotic” for the field, as hundreds of papers released results with different data, algorithms, base models, and even evaluation, making it hard to cross-compare the literature. I expect that the community will soon converge to some standard data/evaluation and we can develop better instruction-tuning models in a more scientific and reproducible way!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Acknowledgement&lt;/strong&gt;: Thanks Tanya Goyal, Zhiyuan Zeng, Mengzhou Xia, Shunyu Yao, and Ben Eysenbach for their helpful feedback on the blogpost!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Open-ended instruction tuning should be distinguished from (multi-task) instruction tuning, which we will explain in details below. Here we use “open-ended instruction tuning” as an umbrella term to cover both supervised fine-tuning and the later alignment stage. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The May2023-version LIMA only beats Alpaca—which imitates davinci003—and davinci003. It’s unclear how LIMA will compare to open-source models using other data. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;WebGPT (&lt;a href=&quot;https://arxiv.org/abs/2112.09332&quot;&gt;Nakano et al., 2021&lt;/a&gt;) also provides a preference data but it is tailored just for the WebGPT task format. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This stage also involves “constitutional AI”, as the SFT model is prompted with “principles” when generating preferences. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This method is never clearly mentioned in any paper and is only hinted by &lt;a href=&quot;https://platform.openai.com/docs/model-index-for-researchers&quot;&gt;this OpenAI document&lt;/a&gt;. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;MT-Bench (&lt;a href=&quot;https://arxiv.org/pdf/2306.05685.pdf&quot;&gt;Zheng et al., 2023&lt;/a&gt;) reports several human agreement rate. When including non-tie and tie (counts inconsistent votes as tie), the human agreement is 63%; when only including the non-tie votes, the human agreement rate is 81%. In both cases, it’s lower than the corresponding GPT-4 agreement rate to humans (66% and 85%). &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Disclaimer: this is not a comprehensive review of instruction-tuning or RLHF literature, but a brief introduction of the recent progress (and a little promotion on our work). However, any comments/suggestions are welcome and please feel free to email me (tianyug@princeton.edu) and I would love to have a discussion!</summary></entry><entry><title type="html">MeZO: Fine-Tuning Language Models with Just Forward Passes</title><link href="https://gaotianyu.xyz/blog/2023/11/14/mezo/" rel="alternate" type="text/html" title="MeZO: Fine-Tuning Language Models with Just Forward Passes" /><published>2023-11-14T00:00:00-05:00</published><updated>2023-11-14T00:00:00-05:00</updated><id>https://gaotianyu.xyz/blog/2023/11/14/mezo</id><content type="html" xml:base="https://gaotianyu.xyz/blog/2023/11/14/mezo/">&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2305.17333&quot;&gt;Paper&lt;/a&gt;][&lt;a href=&quot;https://github.com/princeton-nlp/MeZO&quot;&gt;Code&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Fine-tuning pre-trained language models (LMs) was introduced as an effective method to rapidly adapt highly capable models to solve many language tasks, adapt to specialized domains, or incorporate human instructions and preferences. However, as LMs are scaled up, computing gradients for backpropagation requires a prohibitive amounts of memory – in our test, up to 12× the memory required for inference. This is because we need to cache activations during the forward pass, gradients during the backward pass, and, sometimes even store gradient history (e.g., for Adam).&lt;/p&gt;

&lt;p&gt;This work introduces a memory-efficient zeroth-order optimizer, MeZO, that fine-tunes LMs using only forward passes, thereby requiring the same amount of memory as what’s needed for inference. Despite using substantially less memory, MeZO achieves performance within 1% absolute of standard fine-tuning on most tasks. And, contrary to classical theoretical intuitions, MeZO can efficiently optimize models with up to 66B parameters while using fewer GPU-hours than traditional fine-tuning. Moreover, MeZO can optimize non-differentiable objectives like accuracy, F1, or potentially, scores from a reward model.&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/teaser.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;Comparison of MeZO to zero-shot, in-context learning (ICL), and standard fine-tuning of OPT-13B. MeZO performs within 1% absolute of fine-tuning on 7 out of11 tasks despite using substantially less memory.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/Untitled.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;Contrary to classical analyses that suggest zeroth-order methods are proportionally slow for more parameters, MeZO successfully optimizes OPT models of up to 66B and outperforms zero-shot and in-context learning.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/Untitled%201.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;Unlike backpropagation, MeZO can optimize any objective function, even the non-differentiable ones. For classification tasks (512 examples per class), we use accuracy as the objective; for SQuAD (1,000 examples), we use F1 as the objective.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Fine-tuning with Adam can require up to 12x the memory needed for inference. As a result, one usually needs a lot more GPUs to fine-tune a model than what’s needed to serve it. Even parameter-efficient fine-tuning methods, like prefix tuning and LoRA, require much more memory than inference does. This makes it difficult to fine-tune large, performant pre-trained LLMs.&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/Untitled%202.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;The figure and table above show GPU profiling results of OPT models with different sizes and different methods. Profiling is done on the MultiRC dataset (400 tokens per example on average).&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Other disadvantages to fine-tuning via backpropagation include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Differentiable objective&lt;/strong&gt;: Running backpropagation requires a differentiable objective. This has driven the paradigm of minimizing surrogate differentiable losses instead of directly maximizing accuracy. Analogously, when incorporating human feedback and preferences, one often has to resort to reinforcement learning (&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Ouyang et al., 2022&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication overhead&lt;/strong&gt;: Large models require training on more than 1 GPU. In this case, model weights, activations, and gradients must be frequently synced across GPUs, which introduces a large communication overhead.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradient history&lt;/strong&gt;: Optimizers like Adam and SGD with momentum require access to past gradients in order to update the model. Storing these gradients directly further increases the memory cost.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mezo-algorithm&quot;&gt;MeZO Algorithm&lt;/h2&gt;

&lt;p&gt;MeZO computes gradients using a classical zeroth-order method called SPSA (&lt;a href=&quot;https://ieeexplore.ieee.org/document/119632&quot;&gt;Spall, 1992&lt;/a&gt;). For a loss function $\mathcal{L}(\theta;B)$ evaluated on batch $B$ using model parameters $\theta$, we estimate the gradient as:&lt;/p&gt;

&lt;p&gt;[[
\hat\nabla\mathcal L(\theta; B) = \frac{\mathcal{L}(\theta+\epsilon z; B) - \mathcal{L}(\theta-\epsilon z; B)}{2\epsilon} z 
]]&lt;/p&gt;

&lt;p&gt;where $z$ is an i.i.d. Gaussian perturbation applied element-wise to the parameters and $\epsilon$ controls the strength of the perturbation. This formula can be interpreted as evaluating the gradient in just one direction determined by $z$.&lt;/p&gt;

&lt;p&gt;MeZO requires two forward passes to evaluate the loss function using the perturbed parameters. A straightforward implementation of the algorithm would require storing $z$, which is just as large as the model $\theta$, on the GPU. MeZO adapts this algorithm to operate in-place by saving the random seed and resampling $z$ whenever it’s needed. So, the gradient can be computed and applied to each parameter without consuming any additional memory.&lt;/p&gt;

&lt;p&gt;It is easy to see how MeZO addresses some of the shortcomings of backpropagation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Differentiable objective:&lt;/strong&gt; The gradient estimate only requires evaluating $\mathcal{L}$, so MeZO can optimize non-differentiable objectives. Preliminary experiments in our paper show that that MeZO can directly maximize accuracy on various tasks for models with up to 66B parameters.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication overhead&lt;/strong&gt;: The communication overhead is substantially reduced, because MeZO requires fewer GPUs than standard fine-tuning. As a result, MeZO is more than 7x faster per step and uses half as many GPU-hours as standard fine-tuning on a 30B OPT model. Our paper (Appendix F.6) contains more details about the wall-clock measurement.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradient history&lt;/strong&gt;: MeZO can reconstruct gradients from just two scalars: the normalized difference in the loss values and the random seed used to sample $z$. Accessing the gradient history is thus much more memory-efficient, and MeZO even admits accessing gradients from an arbitrary past step. This also allows us to substantially reduce the checkpoint storage cost compared to parameter-efficient methods: saving a fine-tuned 66B parameter model with MeZO requires less than 0.1 MB whereas LoRA requires 38 MB of storage.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;when-to-use-mezo&quot;&gt;When to use MeZO?&lt;/h2&gt;

&lt;p&gt;You can use MeZO whenever you need to fine-tune a language model with a natural language prompt (the prompt can be as simple as “…. Answer:”). MeZO is especially useful when you are fine-tuning extremely large models and are constrained by GPU memory, as we showed that MeZO can reduce up to 12x memory usage compared to Adam. MeZO may also excel at providing an efficient but fuzzy gradient estimate (e.g., for data selection). Moreover, MeZO can optimize non-differentiable objectives (e.g., human preferences and feedback) as a replacement for more complex algorithms like reinforcement learning.&lt;/p&gt;

&lt;p&gt;You probably don’t want to use MeZO if you are working with very small models (e.g., BERT-size), since MeZO converges much slower compared to backpropagation on small models and can still lag behind backpropagation on certain tasks. We also note that MeZO prescribes a distinct optimization trajectory from standard fine-tuning, so it may not be useful for building a mechanistic interpretation of fine-tuning. Also, MeZO is not likely to perform well when you need to introduce new parameters (e.g., a new prediction head for classification) or when you need to pre-train a model from scratch. As we will explain in the next section, the success of MeZO relies on using a pre-trained base model.&lt;/p&gt;

&lt;h2 id=&quot;why-does-it-work&quot;&gt;Why does it work?&lt;/h2&gt;

&lt;p&gt;Classical analyses (such as &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2012/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html&quot;&gt;Jamieson et al., 2012&lt;/a&gt;; &lt;a href=&quot;https://dl.acm.org/doi/10.1109/TIT.2015.2409256&quot;&gt;Duchi et al., 2015&lt;/a&gt;) have shown that zeroth order methods should require $d$ times as many steps, where $d$ is the number of parameters in the model. For 66B parameter models, this is catastrophic! So, why does MeZO manage to fine-tune models in a reasonable time frame?&lt;/p&gt;

&lt;p&gt;In our paper, we suggest that because the model is pre-trained, when we use a simple prompt (e.g., “… Answer: “) with the downstream task, the loss has an intrinsic low-dimensional structure. Intuitively, a straightforward prompt turns the downstream task into a syntactically correct complete-the-sentence task, so it looks a lot like a pre-training example. This avoids the worst case scenario, where one needs gradient information in all $d$ directions in order to optimize the objective. Instead, the slowdown scales with the intrinsic structure of the task. That’s why a prompt is essential in order for MeZO to work!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Acknowledgement: Thanks to our amazing collaborators: Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. Thanks to Eshaan Nichani and Alex Damian for the feedback on the blog post.&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">[Paper][Code]</summary></entry><entry><title type="html">Seattle</title><link href="https://gaotianyu.xyz/blog/2022/07/31/seattle/" rel="alternate" type="text/html" title="Seattle" /><published>2022-07-31T00:00:00-04:00</published><updated>2022-07-31T00:00:00-04:00</updated><id>https://gaotianyu.xyz/blog/2022/07/31/seattle</id><content type="html" xml:base="https://gaotianyu.xyz/blog/2022/07/31/seattle/">&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00686-2.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00694.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00733-2.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00744.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Hot Air Balloon</title><link href="https://gaotianyu.xyz/blog/2022/07/29/hot_air_balloon/" rel="alternate" type="text/html" title="Hot Air Balloon" /><published>2022-07-29T00:00:00-04:00</published><updated>2022-07-29T00:00:00-04:00</updated><id>https://gaotianyu.xyz/blog/2022/07/29/hot_air_balloon</id><content type="html" xml:base="https://gaotianyu.xyz/blog/2022/07/29/hot_air_balloon/">&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00776.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00786.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00797.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00805.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Paper reading: December 2022 arXiv notes</title><link href="https://gaotianyu.xyz/blog/2022/01/05/dec2022arxiv/" rel="alternate" type="text/html" title="Paper reading: December 2022 arXiv notes" /><published>2022-01-05T00:00:00-05:00</published><updated>2022-01-05T00:00:00-05:00</updated><id>https://gaotianyu.xyz/blog/2022/01/05/dec2022arxiv</id><content type="html" xml:base="https://gaotianyu.xyz/blog/2022/01/05/dec2022arxiv/">&lt;p&gt;I tried to track arXiv papers under cs.CL to learn about the recent progress in NLP. However, such tracking usually breaks when there are a burst of papers, for example, when near an anonymous DDL. My solution to a hundred arXiv tabs opened has been always cmd+Q (a “hard-reboot”, as Southwest Airlines would call it). I realized I always missed piles of interesting papers and relying on Twitter/slack recommendations made me extremely biased of what’s going on. So I decide this time to put in some more effort to go through December’s arXiv papers.&lt;/p&gt;

&lt;p&gt;After EMNLP, NLP forks all left Abu Dhabi except me, staying here to renew my US visa. Sadly, my visa is under “administrative processing” and I have to wait for an “indefinite” period of time. Flying back home (China) is also extremely unaffordable due to lack of flight, so I spent Christmas and New Year wondering around the area, traveling around UAE, Oman, Morocco, and Turkey. While traveling around is lonely, I can also use the time to read some papers and experience being a “digital nomad”.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2023-1-5-dec2022arxiv/muscat.jpeg&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Photo from Muscat, Oman.&lt;/p&gt;

&lt;p&gt;Ok let’s get down to business. This paper list focuses on new papers coming out on arXiv in December 2022 (some might be revision). I am mainly interested in the following directions and the list reflects them too: retrieval (LMs and retrieval; dense retrieval) and LLM (in-context learning; instruction tuning; prompting; generation; efficient use of LLM). Let me know if you find other interesting papers that I didn’t mention!&lt;/p&gt;

&lt;h2 id=&quot;retrievallm-a-path-to-faithful-and-generalizable-llm&quot;&gt;Retrieval+LM: a path to faithful and generalizable LLM?&lt;/h2&gt;

&lt;p&gt;We have seen a lot of hype brought by GPT-3 and ChatGPT, especially how they can answer questions with “facts”, memorized in their parameters. But such behaviors are not generalizable — a model trained in 2022 wouldn’t know any news in 2023 — and also have hallucination issues — the answers are not supported by any explicit documents and may be inaccurate.&lt;/p&gt;

&lt;p&gt;Adding a retrieval component to LMs can be a remedy. We have seen works adding entity knowledge to LMs (&lt;a href=&quot;https://arxiv.org/pdf/1905.07129.pdf&quot;&gt;Zhang et al., 2019&lt;/a&gt;),&lt;/p&gt;</content><author><name></name></author><summary type="html">I tried to track arXiv papers under cs.CL to learn about the recent progress in NLP. However, such tracking usually breaks when there are a burst of papers, for example, when near an anonymous DDL. My solution to a hundred arXiv tabs opened has been always cmd+Q (a “hard-reboot”, as Southwest Airlines would call it). I realized I always missed piles of interesting papers and relying on Twitter/slack recommendations made me extremely biased of what’s going on. So I decide this time to put in some more effort to go through December’s arXiv papers.</summary></entry><entry><title type="html">Prompting: Better Ways of Using Language Models for NLP Tasks</title><link href="https://gaotianyu.xyz/blog/2021/06/02/prompting/" rel="alternate" type="text/html" title="Prompting: Better Ways of Using Language Models for NLP Tasks" /><published>2021-06-02T00:00:00-04:00</published><updated>2021-06-02T00:00:00-04:00</updated><id>https://gaotianyu.xyz/blog/2021/06/02/prompting</id><content type="html" xml:base="https://gaotianyu.xyz/blog/2021/06/02/prompting/">&lt;p&gt;Starting from BERT (&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;Devlin et al., 2019&lt;/a&gt;), fine-tuning pre-trained language models (LMs) with task-specific heads on downstream applications has become standard practice in NLP. However, the GPT-3 model with 175B parameters (&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Brown et al., 2020&lt;/a&gt;) has brought a new way of using LMs for downstream tasks: as the title “Language Models are Few-Shot Learners” suggests, GPT-3 can well handle a wide range of tasks with only a few examples by leveraging natural-language &lt;em&gt;prompts&lt;/em&gt; and task &lt;em&gt;demonstrations&lt;/em&gt; as context, while not updating the parameters in the underlying model. The giant model size of GPT-3 is an important factor for its success, while the concept of prompts and demonstrations also gives us new insights about how we can better use language models.&lt;/p&gt;

&lt;p&gt;So what is a prompt? A prompt is a piece of text inserted in the input examples, so that the original task can be formulated as a (masked) language modeling problem. For example, say we want to classify the sentiment of the movie review “&lt;em&gt;No reason to watch&lt;/em&gt;”, we can append a prompt “It was” to the sentence, getting “&lt;em&gt;No reason to watch.&lt;/em&gt; It was”. It is natural to expect a higher probability from the LM to generate “terrible” than “great” then.&lt;/p&gt;

&lt;p&gt;After the release of GPT-3, many prompt-related papers emerged, and many of them have discussed prompt-based learning for medium-sized pre-trained models like BERT (BERT-base has 110M parameters, 1000x smaller than the largest GPT-3). In this blog post, I will provide an overview of recent prompt-based methods and my perspective of prompting. At the end of it, I am going to introduce our ACL’21 paper, “&lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;&lt;strong&gt;Making Pre-trained Language Models Better Few-shot Learners&lt;/strong&gt;&lt;/a&gt;.”&lt;/p&gt;

&lt;h2 id=&quot;why-we-want-prompts&quot;&gt;Why we want prompts&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;An illustration for pre-training, standard fine-tuning and prompt-based fine-tuning with demonstrations, taking a sentiment classification task as an example (from Gao et al., 2021).&lt;/p&gt;

&lt;p&gt;In the standard “pre-training and fine-tuning” paradigm, the gap between the pre-training stage and the downstream task can be significant: the objectives are different, and for the downstream tasks, we usually need to introduce new parameters—for example, for a BERT-large model and a binary classification task, it requires an additional set of 1,024 x 2 parameters. On the other hand, prompting makes it possible for downstream tasks to take the same format as the pre-training objectives, as illustrated in the above figure, and requires no new parameters. For a classification task, we just need to design a &lt;em&gt;template&lt;/em&gt; (“It was”) and the expected text responses (we call these &lt;em&gt;label words&lt;/em&gt;, e.g., “great” for the positive label and “terrible” for the negative label in the figure). By closing the gap between the two stages, deploying the pre-trained models on specific tasks becomes much easier, especially for the &lt;strong&gt;few-shot&lt;/strong&gt; case—when you only have a dozen of training examples for a new task, it is hard to fine-tune the pre-trained models and the new task-specific parameters effectively, but the process is much smoother with prompting. &lt;a href=&quot;http://arxiv.org/abs/2103.08493&quot;&gt;Scao and Rush (2021)&lt;/a&gt; show that a prompt may be worth 100 conventional data points, suggesting that prompts can bring a giant leap in sample efficiency.&lt;/p&gt;

&lt;p&gt;There are two different paradigms in the research of prompts, and they share different views: Inspired by the PET papers (&lt;a href=&quot;http://arxiv.org/abs/2001.07676&quot;&gt;Schick and Schütze, 2021a&lt;/a&gt;,&lt;a href=&quot;http://arxiv.org/abs/2009.07118&quot;&gt; b&lt;/a&gt;), prompt-based fine-tuning (the critical point is that we still further optimize the parameters) is regarded as a path towards better few-shot learners for small language models (by small, I mean millions instead of billions of parameters, like BERT or RoBERTa); For super-large models like 175B GPT-3 and 11B T5 (&lt;a href=&quot;http://arxiv.org/abs/1910.10683&quot;&gt;Raffel et al., 2020&lt;/a&gt;), since fine-tuning them is hard (this is just my guess, and I never had the chance to do so) and also costly, it is expected instead to fix their parameters and apply them to different tasks by different prompts (either discrete ones or soft ones, which I will talk about later).&lt;/p&gt;

&lt;h2 id=&quot;discrete-prompts&quot;&gt;Discrete prompts&lt;/h2&gt;

&lt;p&gt;The earliest work of using prompts in pre-trained models traces back to GPT-1/2 (&lt;a href=&quot;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;Radford et al., 2018&lt;/a&gt;, &lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;2019&lt;/a&gt;), where the authors show that by designing appropriate prompts, LMs can achieve decent zero-shot performance on tasks from sentiment classification to reading comprehension. Later, &lt;a href=&quot;http://arxiv.org/abs/1909.01066&quot;&gt;Petroni et al. (2019)&lt;/a&gt;;&lt;a href=&quot;https://arxiv.org/abs/1909.00505&quot;&gt; Davison et al. (2019)&lt;/a&gt;;&lt;a href=&quot;http://arxiv.org/abs/1911.12543&quot;&gt; Jiang et al. (2020)&lt;/a&gt;;&lt;a href=&quot;http://arxiv.org/abs/1912.13283&quot;&gt; Talmor et al. (2020)&lt;/a&gt; explore utilizing prompts to mine factual or commonsense knowledge from LMs. After GPT-3 took prompts while fixing the parameters, prompt-based methods were further introduced to smaller LMs (&lt;a href=&quot;http://arxiv.org/abs/2001.07676&quot;&gt;Schick and Schütze, 2021a&lt;/a&gt;,&lt;a href=&quot;http://arxiv.org/abs/2009.07118&quot;&gt; b&lt;/a&gt;; our work LM-BFF, &lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;Gao et al., 2021&lt;/a&gt;). They differ from GPT-3 in that they fine-tune the full model and take bidirectional masked LMs instead of unidirectional LMs. Several recent papers follow this line of approaches by adjusting the objective (&lt;a href=&quot;https://arxiv.org/abs/2103.11955&quot;&gt;Tam et al., 2021&lt;/a&gt;) or formulating tasks in a unified form, like question answering (&lt;a href=&quot;https://arxiv.org/abs/2104.04670&quot;&gt;Zhong et al., 2021a&lt;/a&gt;) or textual entailment (&lt;a href=&quot;http://arxiv.org/abs/2104.14690&quot;&gt;Wang et al., 2021&lt;/a&gt;). With a comprehensive study of different variants, &lt;a href=&quot;https://arxiv.org/pdf/2106.13353.pdf&quot;&gt;Logan et al. (2021)&lt;/a&gt; show that when taking prompt-based fine-tuning (instead of freezing all the parameters), the model can also achieve better performance than standard fine-tuning without prompts (but a good prompt still makes significant differences), and tuning only part of the model parameters—for example, taking a recently-proposed bias tuning method (&lt;a href=&quot;https://arxiv.org/abs/2106.10199&quot;&gt;Ben-Zaken et al., 2021&lt;/a&gt;)—is comparable to full model fine-tuning in the few-shot setting.&lt;/p&gt;

&lt;p&gt;In all those models, prompts are in natural language and are composed of discrete tokens from the vocabulary. Most of the work takes manually-designed prompts—prompt engineering is non-trivial since a small perturbation can significantly affect the model’s performance, and creating a perfect prompt requires both understanding of LMs’ inner workings and trial-and-error.&lt;/p&gt;

&lt;p&gt;In contrast to manually-designed prompts, one can also generate or optimize the prompts: &lt;a href=&quot;https://arxiv.org/abs/2106.07704&quot;&gt;Guo et al., 2021&lt;/a&gt; show a soft Q-learning method that works well for prompt generation; AutoPrompt (&lt;a href=&quot;http://arxiv.org/abs/2010.15980&quot;&gt;Shin et al., 2020&lt;/a&gt;) proposes taking a gradient-based search (the idea was from&lt;a href=&quot;https://arxiv.org/abs/1908.07125&quot;&gt; Wallace et al., 2019&lt;/a&gt;, which aims for searching a universal adversarial trigger to make a model generate a specific prediction) to find out the best prompt for particular tasks. The setting for AutoPrompt is different in that it fixes the models: it just assumes that everything is encoded in the pre-trained models and all we need is to “prompt” it out; another reason is that AutoPrompt also aims for LAMA (&lt;a href=&quot;http://arxiv.org/abs/1909.01066&quot;&gt;Petroni et al., 2019&lt;/a&gt;), a knowledge probing task, where it is required not to touch the model parameters. Below is an example of AutoPrompt used in sentiment classification.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-1.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;An illustration of AutoPrompt (Shin et al., 2020).&lt;/p&gt;

&lt;p&gt;The searched templates considerably improve the LAMA performance; they also achieve surprising accuracies in sentiment classification and natural language inference tasks using the full dataset (still, lower than the fine-tuning paradigm). Taking a look at the searched discrete (but not natural-language anymore) prompts, you can find explanations for some of the “trigger tokens”, but many others are just peculiarities. It is not clear that whether the auto prompt really helps the LMs recall the “knowledge” inside or it is just an alternative way for optimization, picking the “winning tickets” from “lotteries” in the pre-trained models (for the lottery ticket hypothesis, see &lt;a href=&quot;http://arxiv.org/abs/1803.03635&quot;&gt;Frankle and Carbin, 2019&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;soft-prompts-do-we-really-need-discrete-words-in-the-prompts&quot;&gt;Soft prompts: do we really need discrete words in the prompts?&lt;/h2&gt;

&lt;p&gt;Since AutoPrompt already does the gradient-based search for prompts, why not move on from discrete tokens to continuous “soft prompts”? For example, &lt;a href=&quot;http://arxiv.org/abs/2104.05240&quot;&gt;Zhong et al. (2021b)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2104.06599&quot;&gt;Qin and Eisner (2021)&lt;/a&gt; propose to use “soft prompts” for knowledge probing tasks (LAMA etc.) and have achieved considerable improvements over discrete prompts. The idea is so simple—just putting some random vectors (not tied to specific word embeddings from the vocabulary) in the input sequence and tuning them, with other parts of the pre-trained models fixed.&lt;/p&gt;

&lt;p&gt;There is also work deploying soft prompts beyond probing tasks: &lt;a href=&quot;http://arxiv.org/abs/2101.00190&quot;&gt;Li and Liang (2021)&lt;/a&gt; extend the idea to generation tasks and show that it performs on par with fine-tuning while only tuning 0.1% parameters. &lt;a href=&quot;https://arxiv.org/abs/2105.11259&quot;&gt;Han et al. (2021)&lt;/a&gt; combine soft prompts with manual templates and have achieved supreme performance in relation extraction. The most comprehensive study on soft prompts I have seen till now comes from &lt;a href=&quot;http://arxiv.org/abs/2104.08691&quot;&gt;Lester et al. (2021)&lt;/a&gt;: they apply soft prompt on T5 and show that by just tuning the prompt (which only takes a tiny proportion of the total parameters), T5 can achieve on par performance on NLU tasks with fine-tuning the whole model. I also like the paper for that it carries out an extensive ablation study and shows several crucial empirical choices for successful soft prompts, including initialization from word embeddings, enough numbers of soft prompt tokens, and an aligned pre-training objective. Besides parameter efficiency, &lt;a href=&quot;http://arxiv.org/abs/2104.08691&quot;&gt;Lester et al. (2021)&lt;/a&gt; also demonstrate that the soft prompt delivers better transferability than full model fine-tuning.&lt;/p&gt;

&lt;p&gt;Let’s review the idea of soft prompts: it works amazingly well and is especially effective when you cannot (probing tasks) or would not (the model is too large or you want a universal model for all tasks) touch the models’ parameters. Tuning soft prompts is very different from prompt-based fine-tuning, which allows one to optimize the full model and, more importantly, handle few-shot cases much better than standard fine-tuning. Unlike its manual counterpart, AutoPrompt does not work well in the few-shot case, and to the best of my knowledge, no soft-prompt papers argue that they achieve superb few-shot performance (though &lt;a href=&quot;https://arxiv.org/abs/2103.10385&quot;&gt;Liu et al. (2021)&lt;/a&gt; showed satisfying few-shot results by starting from discrete manual prompts and fine-tuning the whole model). Also, as &lt;a href=&quot;http://arxiv.org/abs/2104.08691&quot;&gt;Lester et al. (2021)&lt;/a&gt; demonstrate, soft prompts never achieve the same performance as full fine-tuning on SuperGLUE until using pre-trained models with more than &lt;strong&gt;10 billion parameters&lt;/strong&gt;! I believe it is worth investigating how to push soft prompts further to work more effectively in few-shot cases and smaller language models.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-3.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;50%&quot; /&gt;&lt;br /&gt;GPT-3 (blue) vs full model fine-tuning (orange) vs soft-prompt tuning (green). Credit to Lester et al. (2021).&lt;/p&gt;

&lt;h2 id=&quot;in-context-learning-a-new-form-of-meta-learning&quot;&gt;In-context learning: a new form of meta-learning&lt;/h2&gt;

&lt;p&gt;I attribute GPT-3’s success to two model designs at the beginning of this post: prompts and &lt;em&gt;demonstrations&lt;/em&gt; (or &lt;em&gt;in-context learning&lt;/em&gt;), but I haven’t talked about in-context learning until this section. Since GPT-3’s parameters are not fine-tuned on downstream tasks, it has to “learn” new tasks in an alternative way—through context.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-4.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;50%&quot; /&gt;&lt;br /&gt;GPT-3 &quot;learns&quot; about new tasks through demonstrations in the context (Brown et al., 2020).&lt;/p&gt;

&lt;p&gt;As shown in the above figure, GPT-3 simply concatenates some random examples from the training set with the actual query (“cheese ⇒” in this example), and since the pre-trained model already learns to capture the patterns from the context and Transformers’ self-attention allows token-by-token comparison across these instances, in-context learning works surprisingly well. The GPT-3 paper calls it “meta-learning”, arguing that after reading a large amount of unsupervised text, a language model can “develop a broad set of skills and pattern recognition abilities.” The authors assume that there are “sometimes repeated sub-tasks embedded within a single sequence” during pre-training, similar to the paradigm of in-context learning. Following-up works further refine the way of using demonstrations: &lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;Gao et al., 2021&lt;/a&gt;; &lt;a href=&quot;http://arxiv.org/abs/2101.06804&quot;&gt;Liu et al. (2021)&lt;/a&gt; say that instead of randomly sampling some examples, taking in-context demonstrations similar to the query can substantially improve the performance; &lt;a href=&quot;http://arxiv.org/abs/2104.08786&quot;&gt;Lu et al. (2021)&lt;/a&gt; show that even the order of the demonstrations matters a lot and propose a way to determine the “optimal” order.&lt;/p&gt;

&lt;p&gt;Although in-context learning is only “necessary” when you cannot tune the model, and it is hard to generalize when the number of training examples increases (because the input length of the model is limited), studying how to better use demonstrations (i.e., how to further squeeze “meta-knowledge” learned by LMs) and what pre-training objectives and data can boost in-context abilities may further help us understand pre-trained LMs’ inner workings.&lt;/p&gt;

&lt;h2 id=&quot;calibrating-language-models&quot;&gt;Calibrating language models&lt;/h2&gt;

&lt;p&gt;Prompting is great, but it can also bring bias from the pre-training corpora. For example, in a zero-shot sentiment classification setting, given “N/A” as the input, GPT-3 tends to predict “positive” over “negative”, while it is expected to assign 50/50 probabilities to the two contrastive labels (&lt;a href=&quot;http://arxiv.org/abs/2102.09690&quot;&gt;Zhao et al., 2021&lt;/a&gt;). Another problem is that different surface forms of the same object (e.g., “computer” and “PC”) may compete for the probability mass, leading to undesirable distributions over task labels (&lt;a href=&quot;https://arxiv.org/abs/2104.08315&quot;&gt;Holtzman et al., 2021&lt;/a&gt;). The solution given by &lt;a href=&quot;http://arxiv.org/abs/2102.09690&quot;&gt;Zhao et al. (2021)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2104.08315&quot;&gt;Holtzman et al. (2021)&lt;/a&gt; is &lt;em&gt;calibration&lt;/em&gt;: adding compensation to the biased tokens so that they are calibrated to an unbiased status.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-true-few-shot-setting&quot;&gt;What is a true few-shot setting?&lt;/h2&gt;

&lt;p&gt;There are many discussions about the few-shot setting itself: it is well known that fine-tuning on small datasets can suffer from instability (&lt;a href=&quot;http://arxiv.org/abs/2002.06305&quot;&gt;Dodge et al., 2020&lt;/a&gt;;&lt;a href=&quot;http://arxiv.org/abs/2006.05987&quot;&gt; Zhang et al., 2021&lt;/a&gt;), and different splits of data may affect the performance drastically. Previous works take on various settings, but to account for the large variance in few-shot, multiple sampled few-shot data splits and multiple trials with different seeds are needed to deliver a rigorous and faithful few-shot evaluation (which is what we did in our work). Another problem that is constantly neglected is that one cannot assume a large development set in the few-shot case. To cope with that, &lt;a href=&quot;http://arxiv.org/abs/2009.07118&quot;&gt;Schick and Schütze (2021)&lt;/a&gt; take no dev set and adopt fixed hyper-parameters (which is akin to “shooting in the dark” in such a fluctuating setting and could have unintuitive outcomes), and in our work, we sample a few-shot dev set with the same size as the training set, so we can tune hyper-parameters while keeping it “few-shot”.&lt;/p&gt;

&lt;p&gt;In a recent paper, &lt;a href=&quot;http://arxiv.org/abs/2105.11447&quot;&gt;Perez et al. (2021)&lt;/a&gt; argue that prior work overestimates the few-shot performance of LMs by more or less taking many held-out examples for hyper-parameter tuning, model development or prompt design, and they advocate for a “true few-shot learning” setting. This is consistent with our view that you can only assume few-shot dev examples. However, in the real-world case, one can hardly achieve “true few-shot learning”, for you need an adequate amount of held-out examples to verify that your model is valid on at least one or two tasks. It is a good few-shot model as long as the design can generalize well to other few-shot tasks (these tasks can be so-called “true few-shot”). In our work, we take SST-2 and SNLI for the pilot experiments, and show that our method can well generalize to 13 other NLU tasks.&lt;/p&gt;

&lt;h2 id=&quot;introducing-lm-bff&quot;&gt;Introducing LM-BFF&lt;/h2&gt;

&lt;p&gt;Finally, let me introduce our ACL’21 paper, “&lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;&lt;strong&gt;Making Pre-trained Language Models Better Few-shot Learners&lt;/strong&gt;&lt;/a&gt;”, abbreviated as LM-BFF (better few-shot fine-tuning of language models; alternatively, language models’ best friends forever). LM-BFF is a suite of simple techniques combined for fine-tuning pre-trained LMs on only a small number of training examples, including&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Prompt-based fine-tuning&lt;/strong&gt;, along with a novel method for &lt;strong&gt;automatic prompt generation&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;A dynamic and selective method for incorporating &lt;strong&gt;demonstrations&lt;/strong&gt; in context.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We evaluate LM-BFF in a rigorous few-shot setting (as mentioned above) and show that LM-BFF can drastically outperform standard fine-tuning by up to 30% absolute improvement (on SNLI) and 11% on average. You can find our code at this &lt;a href=&quot;https://github.com/princeton-nlp/lm-bff&quot;&gt;github repo&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prompt-based-fine-tuning&quot;&gt;Prompt-based fine-tuning&lt;/h3&gt;

&lt;p&gt;I have already discussed what prompt-based fine-tuning is—formulating the task as a (masked) language modeling problem with &lt;em&gt;templates&lt;/em&gt; and setting the expected output for each class as &lt;em&gt;label words&lt;/em&gt;. We design manual templates and labels words as listed below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-5.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Manual prompts (templates + label words) used in our experiments. S1 and S2 represent the input sentences.&lt;/p&gt;

&lt;p&gt;However, hand-crafting good prompts can be tricky, requiring domain expertise, and the outcome can be unintuitive. In the following table, we show how sensitive few-shot models are to the small perturbations in the prompt.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-6.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;50%&quot; /&gt;&lt;br /&gt;The impact of different templates and label words. We take RoBERTa-large (Liu et al., 2019) and 16 train/dev examples for each class.&lt;/p&gt;

&lt;p&gt;We observe that if the template is fixed, the better the label words match the “semantic classes”, the better the result is. For example, for SST-2, great/terrible &amp;gt; good/bad &amp;gt; cat/dog &amp;gt; dot/cat &amp;gt; terrible/good (although it’s unclear why RoBERTa thinks 🐱 is more positive than 🐶). From SNLI, we see that if we put [MASK] at the end or swap the two sentences, there could be a &amp;gt;10% performance drop. This urges us to find a better way beyond manual prompts—automatic prompt search.&lt;/p&gt;

&lt;h3 id=&quot;automatic-prompt-search&quot;&gt;Automatic prompt search&lt;/h3&gt;

&lt;p&gt;We separate the automatic prompt search into two parts—automatically searching label words and searching templates.&lt;/p&gt;

&lt;p&gt;For automatic label word search, our goal is to find a set of label words that can maximize the dev performance, &lt;em&gt;given a manual template&lt;/em&gt;. A naive way to do so is to brute-force search all combinations of words. However, it does not work since the search space is exponential in the number of classes, and the method is prone to uncover spurious correlations and overfit. Instead, we first construct a candidate word set $V^c$ for each class $c$: denote $D_{train}^c$ as all the training examples with class $c$, we find the top-k words that maximize the LM probability at [MASK] given the template and $D_{train}^c$. Then we enumerate all word combinations of $V_c$ and find the top-n combinations that maximize zero-shot accuracy on the training set. In the end, we fine-tune all the n combinations and rerank them by the dev performance. We find that both brute-force search in the pruned space and the fine-tuning reranking are quite helpful regarding the final performance.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-7.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;70%&quot; /&gt;&lt;br /&gt;Our approach for template generation.&lt;/p&gt;

&lt;p&gt;For automatic template search, the goal is similar: find the template that maximizes the dev accuracy, &lt;em&gt;given the manual label words&lt;/em&gt;. We use T5 to generate many template candidates in an &lt;em&gt;out-of-the-box manner&lt;/em&gt;, and then rerank them by fine-tuning and dev performance. T5 is a seq-to-seq model and is pre-trained with a fill-in-the-blank objective, making it perfect for generating the template. Take sentiment classification (figure above) as an example, we concatenate the input example and the corresponding label word, and insert &lt;X&gt; and &lt;Y&gt; (mask tokens for T5) around the label word. Note that we want the T5 model to generate conditioned on all the few-shot training examples, so at each position, we take the sum of the log likelihood of all instances (you can refer to our paper for details). We use beam search with a large width (100) to get a large number of high-quality templates in the end.&lt;/Y&gt;&lt;/X&gt;&lt;/p&gt;

&lt;p&gt;The table below shows some examples generated by our automatic prompt search. You can see that for automatic template search, most templates fit the context and the manual label words well, although there are some potentially biased ones (e.g., “no” in SNLI templates). Although mostly looking intuitive, the label-word results do contain some mysterious abnormalities (e.g., “Hi” for the entailment class in SNLI).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-8.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Automatic prompt search results.&lt;/p&gt;

&lt;h3 id=&quot;incorporate-demonstrations&quot;&gt;Incorporate demonstrations&lt;/h3&gt;

&lt;p&gt;I have introduced how GPT-3 uses demonstrations in context: randomly sampling examples from the training set and concatenating them in arbitrary order, which is problematic in many aspects: the input lengths of pre-trained LMs are limited, especially for smaller ones (usually 512); it is hard to grasp meaningful patterns if the examples are concatenated in random orders; demonstrations that look far dissimilar to the input instance may be unhelpful or even cause confusion. Thus we propose this dynamic and selective way of incorporating demonstrations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;During both training and inference, we randomly sample one example for each class from the training set and concatenate them (the first figure in this post gives an example). For inference, we sample multiple sets of demonstrations and ensemble the results in the end.&lt;/li&gt;
  &lt;li&gt;We only sample demonstrations that are closely related to the input. For example, if the input is a movie review, then sampling a movie review is much more helpful than a restaurant review. We take SBERT (&lt;a href=&quot;https://arxiv.org/abs/1908.10084&quot;&gt;Reimers and Gurevych, 2019&lt;/a&gt;) to encode the sentences, calculate the cosine similarities between the input and all the examples from the training set, and only sample from the top 50% examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-9.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Our main results (RoBERTa-large; each class has 16 training examples; results (standard deviation) are averaged over five splits). &quot;GPT-3&quot; in-context learning: use demonstrations in the GPT-3 style, but still take the fixed RoBERTa-large model. FT: fine-tuning; man: manual; auto: automatic templates.&lt;/p&gt;

&lt;p&gt;The above table shows our main experimental results. Here are the main takeaways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using prompts is great, even in the zero-shot case. Also, GPT-3 style learning does not constantly improve the results over the zero-shot model, suggesting that fine-tuning is still needed.&lt;/li&gt;
  &lt;li&gt;Prompt-based fine-tuning is much better than standard fine-tuning, with either manual or automatic prompts. On many tasks, automatic templates can get better results than manual ones.&lt;/li&gt;
  &lt;li&gt;Incorporating demonstrations can further bring significant improvement, showing that even with fine-tuning, adding demonstrations in context can help with the few-shot tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also have many interesting experiments in the paper, showing how automatic prompt generation can be combined with ensemble, and how different demonstration and automatic prompt policies affect the performance. In the end, we show that how the comparison between standard fine-tuning and LM-BFF goes with different numbers of training examples. As it clearly demonstrates, LM-BFF almost saturates its performance on simple tasks like SST-2 with only 32 training examples, and on harder tasks like SNLI, it brings a clear advantage over fine-tuning consistently, until the two converge with nearly one thousand training examples.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-10.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;70%&quot; /&gt;&lt;br /&gt;Standard fine-tuning vs LM-BFF with increasing K (# training examples per class).
&lt;/p&gt;

&lt;p&gt;There are, of course, limitations to our method. There is still large space for improvement in accuracy, and just like standard fine-tuning, LM-BFF is heavily affected by the large variance in few-shot training. Though the automatic prompt achieves on par or better performance than manual ones, it still requires some manual design (auto templates start from manual label words, and auto label words start from manual templates). Finally, prompt-based fine-tuning itself favors certain tasks that (1) can be posed as a “fill-in-the-blank” problem, (2) have relatively short inputs, and (3) do not contain many output classes. These are all open questions left for future work.&lt;/p&gt;

&lt;p&gt;The paper was released at the end of 2020, and there have been lots of exciting advances about few-shot or prompting since then. Nevertheless, LM-BFF is unique in its study in automatic prompt generation and incorporating demonstrations in fine-tuning. Compared to recent soft-prompt approaches, LM-BFF (and other natural-language-prompt-based methods) has a vast advantage in smaller language models and few-shot scenarios. We hope that our work can inspire further exploration in this direction.&lt;/p&gt;

&lt;p&gt;To conclude, in this post I discussed a lot of recent progress about natural-language prompts, soft prompts, and in-context learning, and introduced our LM-BFF paper. I believe prompting is a very promising direction in the years to come. In a broader context, prompt-based methods are about how to better mine the knowledge (about facts, reasoning, understanding sentiment, etc.) from self-supervised learning (pre-training), and efforts in this direction can help squeeze the potentials of LMs and make them better and better learners to our world.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks Danqi Chen and Adam Fisch for proofreading the article and their helpful comments!&lt;/p&gt;</content><author><name></name></author><summary type="html">Starting from BERT (Devlin et al., 2019), fine-tuning pre-trained language models (LMs) with task-specific heads on downstream applications has become standard practice in NLP. However, the GPT-3 model with 175B parameters (Brown et al., 2020) has brought a new way of using LMs for downstream tasks: as the title “Language Models are Few-Shot Learners” suggests, GPT-3 can well handle a wide range of tasks with only a few examples by leveraging natural-language prompts and task demonstrations as context, while not updating the parameters in the underlying model. The giant model size of GPT-3 is an important factor for its success, while the concept of prompts and demonstrations also gives us new insights about how we can better use language models.</summary></entry><entry><title type="html">Forbidden City</title><link href="https://gaotianyu.xyz/blog/2019/12/20/forbidden_city/" rel="alternate" type="text/html" title="Forbidden City" /><published>2019-12-20T00:00:00-05:00</published><updated>2019-12-20T00:00:00-05:00</updated><id>https://gaotianyu.xyz/blog/2019/12/20/forbidden_city</id><content type="html" xml:base="https://gaotianyu.xyz/blog/2019/12/20/forbidden_city/">&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01270.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01166.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01107.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01106.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>