<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-14T11:21:40-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">gaotianyu1350.github.io</title><entry><title type="html">MeZO: Fine-Tuning Language Models with Just Forward Passes</title><link href="http://localhost:4000/blog/2023/11/14/mezo/" rel="alternate" type="text/html" title="MeZO: Fine-Tuning Language Models with Just Forward Passes" /><published>2023-11-14T00:00:00-05:00</published><updated>2023-11-14T00:00:00-05:00</updated><id>http://localhost:4000/blog/2023/11/14/mezo</id><content type="html" xml:base="http://localhost:4000/blog/2023/11/14/mezo/">&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2305.17333&quot;&gt;Paper&lt;/a&gt;][&lt;a href=&quot;https://github.com/princeton-nlp/MeZO&quot;&gt;Code&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Fine-tuning pre-trained language models (LMs) was introduced as an effective method to rapidly adapt highly capable models to solve many language tasks, adapt to specialized domains, or incorporate human instructions and preferences. However, as LMs are scaled up, computing gradients for backpropagation requires a prohibitive amounts of memory ‚Äì in our test, up to 12√ó the memory required for inference. This is because we need to cache activations during the forward pass, gradients during the backward pass, and, sometimes even store gradient history (e.g., for Adam).&lt;/p&gt;

&lt;p&gt;This work introduces a memory-efficient zeroth-order optimizer, MeZO, that fine-tunes LMs using only forward passes, thereby requiring the same amount of memory as what‚Äôs needed for inference. Despite using substantially less memory, MeZO achieves performance within 1% absolute of standard fine-tuning on most tasks. And, contrary to classical theoretical intuitions, MeZO can efficiently optimize models with up to 66B parameters while using fewer GPU-hours than traditional fine-tuning. Moreover, MeZO can optimize non-differentiable objectives like accuracy, F1, or potentially, scores from a reward model.&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/teaser.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;Comparison of MeZO to zero-shot, in-context learning (ICL), and standard fine-tuning of OPT-13B. MeZO performs within 1% absolute of fine-tuning on 7 out of11 tasks despite using substantially less memory.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/Untitled.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;Contrary to classical analyses that suggest zeroth-order methods are proportionally slow for more parameters, MeZO successfully optimizes OPT models of up to 66B and outperforms zero-shot and in-context learning.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/Untitled%201.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;Unlike backpropagation, MeZO can optimize any objective function, even the non-differentiable ones. For classification tasks (512 examples per class), we use accuracy as the objective; for SQuAD (1,000 examples), we use F1 as the objective.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Fine-tuning with Adam can require up to 12x the memory needed for inference. As a result, one usually needs a lot more GPUs to fine-tune a model than what‚Äôs needed to serve it. Even parameter-efficient fine-tuning methods, like prefix tuning and LoRA, require much more memory than inference does. This makes it difficult to fine-tune large, performant pre-trained LLMs.&lt;/p&gt;

&lt;div&gt;&lt;img src=&quot;/assets/mezo/Untitled%202.png&quot; align=&quot;center&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;&lt;p align=&quot;left&quot;&gt;&lt;i&gt;The figure and table above show GPU profiling results of OPT models with different sizes and different methods. Profiling is done on the MultiRC dataset (400 tokens per example on average).&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Other disadvantages to fine-tuning via backpropagation include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Differentiable objective&lt;/strong&gt;: Running backpropagation requires a differentiable objective. This has driven the paradigm of minimizing surrogate differentiable losses instead of directly maximizing accuracy. Analogously, when incorporating human feedback and preferences, one often has to resort to reinforcement learning (&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Ouyang et al., 2022&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication overhead&lt;/strong&gt;: Large models require training on more than 1 GPU. In this case, model weights, activations, and gradients must be frequently synced across GPUs, which introduces a large communication overhead.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradient history&lt;/strong&gt;: Optimizers like Adam and SGD with momentum require access to past gradients in order to update the model. Storing these gradients directly further increases the memory cost.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mezo-algorithm&quot;&gt;MeZO Algorithm&lt;/h2&gt;

&lt;p&gt;MeZO computes gradients using a classical zeroth-order method called SPSA (&lt;a href=&quot;https://ieeexplore.ieee.org/document/119632&quot;&gt;Spall, 1992&lt;/a&gt;). For a loss function $\mathcal{L}(\theta;B)$ evaluated on batch $B$ using model parameters $\theta$, we estimate the gradient as:&lt;/p&gt;

&lt;p&gt;[[
\hat\nabla\mathcal L(\theta; B) = \frac{\mathcal{L}(\theta+\epsilon z; B) - \mathcal{L}(\theta-\epsilon z; B)}{2\epsilon} z 
]]&lt;/p&gt;

&lt;p&gt;where $z$ is an i.i.d. Gaussian perturbation applied element-wise to the parameters and $\epsilon$ controls the strength of the perturbation. This formula can be interpreted as evaluating the gradient in just one direction determined by $z$.&lt;/p&gt;

&lt;p&gt;MeZO requires two forward passes to evaluate the loss function using the perturbed parameters. A straightforward implementation of the algorithm would require storing $z$, which is just as large as the model $\theta$, on the GPU. MeZO adapts this algorithm to operate in-place by saving the random seed and resampling $z$ whenever it‚Äôs needed. So, the gradient can be computed and applied to each parameter without consuming any additional memory.&lt;/p&gt;

&lt;p&gt;It is easy to see how MeZO addresses some of the shortcomings of backpropagation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Differentiable objective:&lt;/strong&gt; The gradient estimate only requires evaluating $\mathcal{L}$, so MeZO can optimize non-differentiable objectives. Preliminary experiments in our paper show that that MeZO can directly maximize accuracy on various tasks for models with up to 66B parameters.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication overhead&lt;/strong&gt;: The communication overhead is substantially reduced, because MeZO requires fewer GPUs than standard fine-tuning. As a result, MeZO is more than 7x faster per step and uses half as many GPU-hours as standard fine-tuning on a 30B OPT model. Our paper (Appendix F.6) contains more details about the wall-clock measurement.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradient history&lt;/strong&gt;: MeZO can reconstruct gradients from just two scalars: the normalized difference in the loss values and the random seed used to sample $z$. Accessing the gradient history is thus much more memory-efficient, and MeZO even admits accessing gradients from an arbitrary past step. This also allows us to substantially reduce the checkpoint storage cost compared to parameter-efficient methods: saving a fine-tuned 66B parameter model with MeZO requires less than 0.1 MB whereas LoRA requires 38 MB of storage.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;when-to-use-mezo&quot;&gt;When to use MeZO?&lt;/h2&gt;

&lt;p&gt;You can use MeZO whenever you need to fine-tune a language model with a natural language prompt (the prompt can be as simple as ‚Äú‚Ä¶. Answer:‚Äù). MeZO is especially useful when you are fine-tuning extremely large models and are constrained by GPU memory, as we showed that MeZO can reduce up to 12x memory usage compared to Adam. MeZO may also excel at providing an efficient but fuzzy gradient estimate (e.g., for data selection). Moreover, MeZO can optimize non-differentiable objectives (e.g., human preferences and feedback) as a replacement for more complex algorithms like reinforcement learning.&lt;/p&gt;

&lt;p&gt;You probably don‚Äôt want to use MeZO if you are working with very small models (e.g., BERT-size), since MeZO converges much slower compared to backpropagation on small models and can still lag behind backpropagation on certain tasks. We also note that MeZO prescribes a distinct optimization trajectory from standard fine-tuning, so it may not be useful for building a mechanistic interpretation of fine-tuning. Also, MeZO is not likely to perform well when you need to introduce new parameters (e.g., a new prediction head for classification) or when you need to pre-train a model from scratch. As we will explain in the next section, the success of MeZO relies on using a pre-trained base model.&lt;/p&gt;

&lt;h2 id=&quot;why-does-it-work&quot;&gt;Why does it work?&lt;/h2&gt;

&lt;p&gt;Classical analyses (such as &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2012/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html&quot;&gt;Jamieson et al., 2012&lt;/a&gt;; &lt;a href=&quot;https://dl.acm.org/doi/10.1109/TIT.2015.2409256&quot;&gt;Duchi et al., 2015&lt;/a&gt;) have shown that zeroth order methods should require $d$ times as many steps, where $d$ is the number of parameters in the model. For 66B parameter models, this is catastrophic! So, why does MeZO manage to fine-tune models in a reasonable time frame?&lt;/p&gt;

&lt;p&gt;In our paper, we suggest that because the model is pre-trained, when we use a simple prompt (e.g., ‚Äú‚Ä¶ Answer: ‚Äú) with the downstream task, the loss has an intrinsic low-dimensional structure. Intuitively, a straightforward prompt turns the downstream task into a syntactically correct complete-the-sentence task, so it looks a lot like a pre-training example. This avoids the worst case scenario, where one needs gradient information in all $d$ directions in order to optimize the objective. Instead, the slowdown scales with the intrinsic structure of the task. That‚Äôs why a prompt is essential in order for MeZO to work!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Acknowledgement: Thanks to our amazing collaborators: Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. Thanks to Eshaan Nichani and Alex Damian for the feedback on the blog post.&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">[Paper][Code]</summary></entry><entry><title type="html">Seattle</title><link href="http://localhost:4000/blog/2022/07/31/seattle/" rel="alternate" type="text/html" title="Seattle" /><published>2022-07-31T00:00:00-04:00</published><updated>2022-07-31T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/07/31/seattle</id><content type="html" xml:base="http://localhost:4000/blog/2022/07/31/seattle/">&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00686-2.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00694.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00733-2.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/seattle/DSC00744.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Hot Air Balloon</title><link href="http://localhost:4000/blog/2022/07/29/hot_air_balloon/" rel="alternate" type="text/html" title="Hot Air Balloon" /><published>2022-07-29T00:00:00-04:00</published><updated>2022-07-29T00:00:00-04:00</updated><id>http://localhost:4000/blog/2022/07/29/hot_air_balloon</id><content type="html" xml:base="http://localhost:4000/blog/2022/07/29/hot_air_balloon/">&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00776.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00786.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00797.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/hot_air_balloon/DSC00805.jpg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Paper reading: December 2022 arXiv notes</title><link href="http://localhost:4000/blog/2022/01/05/dec2022arxiv/" rel="alternate" type="text/html" title="Paper reading: December 2022 arXiv notes" /><published>2022-01-05T00:00:00-05:00</published><updated>2022-01-05T00:00:00-05:00</updated><id>http://localhost:4000/blog/2022/01/05/dec2022arxiv</id><content type="html" xml:base="http://localhost:4000/blog/2022/01/05/dec2022arxiv/">&lt;p&gt;I tried to track arXiv papers under cs.CL to learn about the recent progress in NLP. However, such tracking usually breaks when there are a burst of papers, for example, when near an anonymous DDL. My solution to a hundred arXiv tabs opened has been always cmd+Q (a ‚Äúhard-reboot‚Äù, as Southwest Airlines would call it). I realized I always missed piles of interesting papers and relying on Twitter/slack recommendations made me extremely biased of what‚Äôs going on. So I decide this time to put in some more effort to go through December‚Äôs arXiv papers.&lt;/p&gt;

&lt;p&gt;After EMNLP, NLP forks all left Abu Dhabi except me, staying here to renew my US visa. Sadly, my visa is under ‚Äúadministrative processing‚Äù and I have to wait for an ‚Äúindefinite‚Äù period of time. Flying back home (China) is also extremely unaffordable due to lack of flight, so I spent Christmas and New Year wondering around the area, traveling around UAE, Oman, Morocco, and Turkey. While traveling around is lonely, I can also use the time to read some papers and experience being a ‚Äúdigital nomad‚Äù.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2023-1-5-dec2022arxiv/muscat.jpeg&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Photo from Muscat, Oman.&lt;/p&gt;

&lt;p&gt;Ok let‚Äôs get down to business. This paper list focuses on new papers coming out on arXiv in December 2022 (some might be revision). I am mainly interested in the following directions and the list reflects them too: retrieval (LMs and retrieval; dense retrieval) and LLM (in-context learning; instruction tuning; prompting; generation; efficient use of LLM). Let me know if you find other interesting papers that I didn‚Äôt mention!&lt;/p&gt;

&lt;h2 id=&quot;retrievallm-a-path-to-faithful-and-generalizable-llm&quot;&gt;Retrieval+LM: a path to faithful and generalizable LLM?&lt;/h2&gt;

&lt;p&gt;We have seen a lot of hype brought by GPT-3 and ChatGPT, especially how they can answer questions with ‚Äúfacts‚Äù, memorized in their parameters. But such behaviors are not generalizable ‚Äî a model trained in 2022 wouldn‚Äôt know any news in 2023 ‚Äî and also have hallucination issues ‚Äî the answers are not supported by any explicit documents and may be inaccurate.&lt;/p&gt;

&lt;p&gt;Adding a retrieval component to LMs can be a remedy. We have seen works adding entity knowledge to LMs (&lt;a href=&quot;https://arxiv.org/pdf/1905.07129.pdf&quot;&gt;Zhang et al., 2019&lt;/a&gt;),&lt;/p&gt;</content><author><name></name></author><summary type="html">I tried to track arXiv papers under cs.CL to learn about the recent progress in NLP. However, such tracking usually breaks when there are a burst of papers, for example, when near an anonymous DDL. My solution to a hundred arXiv tabs opened has been always cmd+Q (a ‚Äúhard-reboot‚Äù, as Southwest Airlines would call it). I realized I always missed piles of interesting papers and relying on Twitter/slack recommendations made me extremely biased of what‚Äôs going on. So I decide this time to put in some more effort to go through December‚Äôs arXiv papers.</summary></entry><entry><title type="html">Prompting: Better Ways of Using Language Models for NLP Tasks</title><link href="http://localhost:4000/blog/2021/06/02/prompting/" rel="alternate" type="text/html" title="Prompting: Better Ways of Using Language Models for NLP Tasks" /><published>2021-06-02T00:00:00-04:00</published><updated>2021-06-02T00:00:00-04:00</updated><id>http://localhost:4000/blog/2021/06/02/prompting</id><content type="html" xml:base="http://localhost:4000/blog/2021/06/02/prompting/">&lt;p&gt;Starting from BERT (&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;Devlin et al., 2019&lt;/a&gt;), fine-tuning pre-trained language models (LMs) with task-specific heads on downstream applications has become standard practice in NLP. However, the GPT-3 model with 175B parameters (&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Brown et al., 2020&lt;/a&gt;) has brought a new way of using LMs for downstream tasks: as the title ‚ÄúLanguage Models are Few-Shot Learners‚Äù suggests, GPT-3 can well handle a wide range of tasks with only a few examples by leveraging natural-language &lt;em&gt;prompts&lt;/em&gt; and task &lt;em&gt;demonstrations&lt;/em&gt; as context, while not updating the parameters in the underlying model. The giant model size of GPT-3 is an important factor for its success, while the concept of prompts and demonstrations also gives us new insights about how we can better use language models.&lt;/p&gt;

&lt;p&gt;So what is a prompt? A prompt is a piece of text inserted in the input examples, so that the original task can be formulated as a (masked) language modeling problem. For example, say we want to classify the sentiment of the movie review ‚Äú&lt;em&gt;No reason to watch&lt;/em&gt;‚Äù, we can append a prompt ‚ÄúIt was‚Äù to the sentence, getting ‚Äú&lt;em&gt;No reason to watch.&lt;/em&gt; It was‚Äù. It is natural to expect a higher probability from the LM to generate ‚Äúterrible‚Äù than ‚Äúgreat‚Äù then.&lt;/p&gt;

&lt;p&gt;After the release of GPT-3, many prompt-related papers emerged, and many of them have discussed prompt-based learning for medium-sized pre-trained models like BERT (BERT-base has 110M parameters, 1000x smaller than the largest GPT-3). In this blog post, I will provide an overview of recent prompt-based methods and my perspective of prompting. At the end of it, I am going to introduce our ACL‚Äô21 paper, ‚Äú&lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;&lt;strong&gt;Making Pre-trained Language Models Better Few-shot Learners&lt;/strong&gt;&lt;/a&gt;.‚Äù&lt;/p&gt;

&lt;h2 id=&quot;why-we-want-prompts&quot;&gt;Why we want prompts&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;An illustration for pre-training, standard fine-tuning and prompt-based fine-tuning with demonstrations, taking a sentiment classification task as an example (from Gao et al., 2021).&lt;/p&gt;

&lt;p&gt;In the standard ‚Äúpre-training and fine-tuning‚Äù paradigm, the gap between the pre-training stage and the downstream task can be significant: the objectives are different, and for the downstream tasks, we usually need to introduce new parameters‚Äîfor example, for a BERT-large model and a binary classification task, it requires an additional set of 1,024 x 2 parameters. On the other hand, prompting makes it possible for downstream tasks to take the same format as the pre-training objectives, as illustrated in the above figure, and requires no new parameters. For a classification task, we just need to design a &lt;em&gt;template&lt;/em&gt; (‚ÄúIt was‚Äù) and the expected text responses (we call these &lt;em&gt;label words&lt;/em&gt;, e.g., ‚Äúgreat‚Äù for the positive label and ‚Äúterrible‚Äù for the negative label in the figure). By closing the gap between the two stages, deploying the pre-trained models on specific tasks becomes much easier, especially for the &lt;strong&gt;few-shot&lt;/strong&gt; case‚Äîwhen you only have a dozen of training examples for a new task, it is hard to fine-tune the pre-trained models and the new task-specific parameters effectively, but the process is much smoother with prompting. &lt;a href=&quot;http://arxiv.org/abs/2103.08493&quot;&gt;Scao and Rush (2021)&lt;/a&gt; show that a prompt may be worth 100 conventional data points, suggesting that prompts can bring a giant leap in sample efficiency.&lt;/p&gt;

&lt;p&gt;There are two different paradigms in the research of prompts, and they share different views: Inspired by the PET papers (&lt;a href=&quot;http://arxiv.org/abs/2001.07676&quot;&gt;Schick and Sch√ºtze, 2021a&lt;/a&gt;,&lt;a href=&quot;http://arxiv.org/abs/2009.07118&quot;&gt; b&lt;/a&gt;), prompt-based fine-tuning (the critical point is that we still further optimize the parameters) is regarded as a path towards better few-shot learners for small language models (by small, I mean millions instead of billions of parameters, like BERT or RoBERTa); For super-large models like 175B GPT-3 and 11B T5 (&lt;a href=&quot;http://arxiv.org/abs/1910.10683&quot;&gt;Raffel et al., 2020&lt;/a&gt;), since fine-tuning them is hard (this is just my guess, and I never had the chance to do so) and also costly, it is expected instead to fix their parameters and apply them to different tasks by different prompts (either discrete ones or soft ones, which I will talk about later).&lt;/p&gt;

&lt;h2 id=&quot;discrete-prompts&quot;&gt;Discrete prompts&lt;/h2&gt;

&lt;p&gt;The earliest work of using prompts in pre-trained models traces back to GPT-1/2 (&lt;a href=&quot;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;Radford et al., 2018&lt;/a&gt;, &lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;2019&lt;/a&gt;), where the authors show that by designing appropriate prompts, LMs can achieve decent zero-shot performance on tasks from sentiment classification to reading comprehension. Later, &lt;a href=&quot;http://arxiv.org/abs/1909.01066&quot;&gt;Petroni et al. (2019)&lt;/a&gt;;&lt;a href=&quot;https://arxiv.org/abs/1909.00505&quot;&gt; Davison et al. (2019)&lt;/a&gt;;&lt;a href=&quot;http://arxiv.org/abs/1911.12543&quot;&gt; Jiang et al. (2020)&lt;/a&gt;;&lt;a href=&quot;http://arxiv.org/abs/1912.13283&quot;&gt; Talmor et al. (2020)&lt;/a&gt; explore utilizing prompts to mine factual or commonsense knowledge from LMs. After GPT-3 took prompts while fixing the parameters, prompt-based methods were further introduced to smaller LMs (&lt;a href=&quot;http://arxiv.org/abs/2001.07676&quot;&gt;Schick and Sch√ºtze, 2021a&lt;/a&gt;,&lt;a href=&quot;http://arxiv.org/abs/2009.07118&quot;&gt; b&lt;/a&gt;; our work LM-BFF, &lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;Gao et al., 2021&lt;/a&gt;). They differ from GPT-3 in that they fine-tune the full model and take bidirectional masked LMs instead of unidirectional LMs. Several recent papers follow this line of approaches by adjusting the objective (&lt;a href=&quot;https://arxiv.org/abs/2103.11955&quot;&gt;Tam et al., 2021&lt;/a&gt;) or formulating tasks in a unified form, like question answering (&lt;a href=&quot;https://arxiv.org/abs/2104.04670&quot;&gt;Zhong et al., 2021a&lt;/a&gt;) or textual entailment (&lt;a href=&quot;http://arxiv.org/abs/2104.14690&quot;&gt;Wang et al., 2021&lt;/a&gt;). With a comprehensive study of different variants, &lt;a href=&quot;https://arxiv.org/pdf/2106.13353.pdf&quot;&gt;Logan et al. (2021)&lt;/a&gt; show that when taking prompt-based fine-tuning (instead of freezing all the parameters), the model can also achieve better performance than standard fine-tuning without prompts (but a good prompt still makes significant differences), and tuning only part of the model parameters‚Äîfor example, taking a recently-proposed bias tuning method (&lt;a href=&quot;https://arxiv.org/abs/2106.10199&quot;&gt;Ben-Zaken et al., 2021&lt;/a&gt;)‚Äîis comparable to full model fine-tuning in the few-shot setting.&lt;/p&gt;

&lt;p&gt;In all those models, prompts are in natural language and are composed of discrete tokens from the vocabulary. Most of the work takes manually-designed prompts‚Äîprompt engineering is non-trivial since a small perturbation can significantly affect the model‚Äôs performance, and creating a perfect prompt requires both understanding of LMs‚Äô inner workings and trial-and-error.&lt;/p&gt;

&lt;p&gt;In contrast to manually-designed prompts, one can also generate or optimize the prompts: &lt;a href=&quot;https://arxiv.org/abs/2106.07704&quot;&gt;Guo et al., 2021&lt;/a&gt; show a soft Q-learning method that works well for prompt generation; AutoPrompt (&lt;a href=&quot;http://arxiv.org/abs/2010.15980&quot;&gt;Shin et al., 2020&lt;/a&gt;) proposes taking a gradient-based search (the idea was from&lt;a href=&quot;https://arxiv.org/abs/1908.07125&quot;&gt; Wallace et al., 2019&lt;/a&gt;, which aims for searching a universal adversarial trigger to make a model generate a specific prediction) to find out the best prompt for particular tasks. The setting for AutoPrompt is different in that it fixes the models: it just assumes that everything is encoded in the pre-trained models and all we need is to ‚Äúprompt‚Äù it out; another reason is that AutoPrompt also aims for LAMA (&lt;a href=&quot;http://arxiv.org/abs/1909.01066&quot;&gt;Petroni et al., 2019&lt;/a&gt;), a knowledge probing task, where it is required not to touch the model parameters. Below is an example of AutoPrompt used in sentiment classification.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-1.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;An illustration of AutoPrompt (Shin et al., 2020).&lt;/p&gt;

&lt;p&gt;The searched templates considerably improve the LAMA performance; they also achieve surprising accuracies in sentiment classification and natural language inference tasks using the full dataset (still, lower than the fine-tuning paradigm). Taking a look at the searched discrete (but not natural-language anymore) prompts, you can find explanations for some of the ‚Äútrigger tokens‚Äù, but many others are just peculiarities. It is not clear that whether the auto prompt really helps the LMs recall the ‚Äúknowledge‚Äù inside or it is just an alternative way for optimization, picking the ‚Äúwinning tickets‚Äù from ‚Äúlotteries‚Äù in the pre-trained models (for the lottery ticket hypothesis, see &lt;a href=&quot;http://arxiv.org/abs/1803.03635&quot;&gt;Frankle and Carbin, 2019&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;soft-prompts-do-we-really-need-discrete-words-in-the-prompts&quot;&gt;Soft prompts: do we really need discrete words in the prompts?&lt;/h2&gt;

&lt;p&gt;Since AutoPrompt already does the gradient-based search for prompts, why not move on from discrete tokens to continuous ‚Äúsoft prompts‚Äù? For example, &lt;a href=&quot;http://arxiv.org/abs/2104.05240&quot;&gt;Zhong et al. (2021b)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2104.06599&quot;&gt;Qin and Eisner (2021)&lt;/a&gt; propose to use ‚Äúsoft prompts‚Äù for knowledge probing tasks (LAMA etc.) and have achieved considerable improvements over discrete prompts. The idea is so simple‚Äîjust putting some random vectors (not tied to specific word embeddings from the vocabulary) in the input sequence and tuning them, with other parts of the pre-trained models fixed.&lt;/p&gt;

&lt;p&gt;There is also work deploying soft prompts beyond probing tasks: &lt;a href=&quot;http://arxiv.org/abs/2101.00190&quot;&gt;Li and Liang (2021)&lt;/a&gt; extend the idea to generation tasks and show that it performs on par with fine-tuning while only tuning 0.1% parameters. &lt;a href=&quot;https://arxiv.org/abs/2105.11259&quot;&gt;Han et al. (2021)&lt;/a&gt; combine soft prompts with manual templates and have achieved supreme performance in relation extraction. The most comprehensive study on soft prompts I have seen till now comes from &lt;a href=&quot;http://arxiv.org/abs/2104.08691&quot;&gt;Lester et al. (2021)&lt;/a&gt;: they apply soft prompt on T5 and show that by just tuning the prompt (which only takes a tiny proportion of the total parameters), T5 can achieve on par performance on NLU tasks with fine-tuning the whole model. I also like the paper for that it carries out an extensive ablation study and shows several crucial empirical choices for successful soft prompts, including initialization from word embeddings, enough numbers of soft prompt tokens, and an aligned pre-training objective. Besides parameter efficiency, &lt;a href=&quot;http://arxiv.org/abs/2104.08691&quot;&gt;Lester et al. (2021)&lt;/a&gt; also demonstrate that the soft prompt delivers better transferability than full model fine-tuning.&lt;/p&gt;

&lt;p&gt;Let‚Äôs review the idea of soft prompts: it works amazingly well and is especially effective when you cannot (probing tasks) or would not (the model is too large or you want a universal model for all tasks) touch the models‚Äô parameters. Tuning soft prompts is very different from prompt-based fine-tuning, which allows one to optimize the full model and, more importantly, handle few-shot cases much better than standard fine-tuning. Unlike its manual counterpart, AutoPrompt does not work well in the few-shot case, and to the best of my knowledge, no soft-prompt papers argue that they achieve superb few-shot performance (though &lt;a href=&quot;https://arxiv.org/abs/2103.10385&quot;&gt;Liu et al. (2021)&lt;/a&gt; showed satisfying few-shot results by starting from discrete manual prompts and fine-tuning the whole model). Also, as &lt;a href=&quot;http://arxiv.org/abs/2104.08691&quot;&gt;Lester et al. (2021)&lt;/a&gt; demonstrate, soft prompts never achieve the same performance as full fine-tuning on SuperGLUE until using pre-trained models with more than &lt;strong&gt;10 billion parameters&lt;/strong&gt;! I believe it is worth investigating how to push soft prompts further to work more effectively in few-shot cases and smaller language models.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-3.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;50%&quot; /&gt;&lt;br /&gt;GPT-3 (blue) vs full model fine-tuning (orange) vs soft-prompt tuning (green). Credit to Lester et al. (2021).&lt;/p&gt;

&lt;h2 id=&quot;in-context-learning-a-new-form-of-meta-learning&quot;&gt;In-context learning: a new form of meta-learning&lt;/h2&gt;

&lt;p&gt;I attribute GPT-3‚Äôs success to two model designs at the beginning of this post: prompts and &lt;em&gt;demonstrations&lt;/em&gt; (or &lt;em&gt;in-context learning&lt;/em&gt;), but I haven‚Äôt talked about in-context learning until this section. Since GPT-3‚Äôs parameters are not fine-tuned on downstream tasks, it has to ‚Äúlearn‚Äù new tasks in an alternative way‚Äîthrough context.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-4.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;50%&quot; /&gt;&lt;br /&gt;GPT-3 &quot;learns&quot; about new tasks through demonstrations in the context (Brown et al., 2020).&lt;/p&gt;

&lt;p&gt;As shown in the above figure, GPT-3 simply concatenates some random examples from the training set with the actual query (‚Äúcheese ‚áí‚Äù in this example), and since the pre-trained model already learns to capture the patterns from the context and Transformers‚Äô self-attention allows token-by-token comparison across these instances, in-context learning works surprisingly well. The GPT-3 paper calls it ‚Äúmeta-learning‚Äù, arguing that after reading a large amount of unsupervised text, a language model can ‚Äúdevelop a broad set of skills and pattern recognition abilities.‚Äù The authors assume that there are ‚Äúsometimes repeated sub-tasks embedded within a single sequence‚Äù during pre-training, similar to the paradigm of in-context learning. Following-up works further refine the way of using demonstrations: &lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;Gao et al., 2021&lt;/a&gt;; &lt;a href=&quot;http://arxiv.org/abs/2101.06804&quot;&gt;Liu et al. (2021)&lt;/a&gt; say that instead of randomly sampling some examples, taking in-context demonstrations similar to the query can substantially improve the performance; &lt;a href=&quot;http://arxiv.org/abs/2104.08786&quot;&gt;Lu et al. (2021)&lt;/a&gt; show that even the order of the demonstrations matters a lot and propose a way to determine the ‚Äúoptimal‚Äù order.&lt;/p&gt;

&lt;p&gt;Although in-context learning is only ‚Äúnecessary‚Äù when you cannot tune the model, and it is hard to generalize when the number of training examples increases (because the input length of the model is limited), studying how to better use demonstrations (i.e., how to further squeeze ‚Äúmeta-knowledge‚Äù learned by LMs) and what pre-training objectives and data can boost in-context abilities may further help us understand pre-trained LMs‚Äô inner workings.&lt;/p&gt;

&lt;h2 id=&quot;calibrating-language-models&quot;&gt;Calibrating language models&lt;/h2&gt;

&lt;p&gt;Prompting is great, but it can also bring bias from the pre-training corpora. For example, in a zero-shot sentiment classification setting, given ‚ÄúN/A‚Äù as the input, GPT-3 tends to predict ‚Äúpositive‚Äù over ‚Äúnegative‚Äù, while it is expected to assign 50/50 probabilities to the two contrastive labels (&lt;a href=&quot;http://arxiv.org/abs/2102.09690&quot;&gt;Zhao et al., 2021&lt;/a&gt;). Another problem is that different surface forms of the same object (e.g., ‚Äúcomputer‚Äù and ‚ÄúPC‚Äù) may compete for the probability mass, leading to undesirable distributions over task labels (&lt;a href=&quot;https://arxiv.org/abs/2104.08315&quot;&gt;Holtzman et al., 2021&lt;/a&gt;). The solution given by &lt;a href=&quot;http://arxiv.org/abs/2102.09690&quot;&gt;Zhao et al. (2021)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2104.08315&quot;&gt;Holtzman et al. (2021)&lt;/a&gt; is &lt;em&gt;calibration&lt;/em&gt;: adding compensation to the biased tokens so that they are calibrated to an unbiased status.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-true-few-shot-setting&quot;&gt;What is a true few-shot setting?&lt;/h2&gt;

&lt;p&gt;There are many discussions about the few-shot setting itself: it is well known that fine-tuning on small datasets can suffer from instability (&lt;a href=&quot;http://arxiv.org/abs/2002.06305&quot;&gt;Dodge et al., 2020&lt;/a&gt;;&lt;a href=&quot;http://arxiv.org/abs/2006.05987&quot;&gt; Zhang et al., 2021&lt;/a&gt;), and different splits of data may affect the performance drastically. Previous works take on various settings, but to account for the large variance in few-shot, multiple sampled few-shot data splits and multiple trials with different seeds are needed to deliver a rigorous and faithful few-shot evaluation (which is what we did in our work). Another problem that is constantly neglected is that one cannot assume a large development set in the few-shot case. To cope with that, &lt;a href=&quot;http://arxiv.org/abs/2009.07118&quot;&gt;Schick and Sch√ºtze (2021)&lt;/a&gt; take no dev set and adopt fixed hyper-parameters (which is akin to ‚Äúshooting in the dark‚Äù in such a fluctuating setting and could have unintuitive outcomes), and in our work, we sample a few-shot dev set with the same size as the training set, so we can tune hyper-parameters while keeping it ‚Äúfew-shot‚Äù.&lt;/p&gt;

&lt;p&gt;In a recent paper, &lt;a href=&quot;http://arxiv.org/abs/2105.11447&quot;&gt;Perez et al. (2021)&lt;/a&gt; argue that prior work overestimates the few-shot performance of LMs by more or less taking many held-out examples for hyper-parameter tuning, model development or prompt design, and they advocate for a ‚Äútrue few-shot learning‚Äù setting. This is consistent with our view that you can only assume few-shot dev examples. However, in the real-world case, one can hardly achieve ‚Äútrue few-shot learning‚Äù, for you need an adequate amount of held-out examples to verify that your model is valid on at least one or two tasks. It is a good few-shot model as long as the design can generalize well to other few-shot tasks (these tasks can be so-called ‚Äútrue few-shot‚Äù). In our work, we take SST-2 and SNLI for the pilot experiments, and show that our method can well generalize to 13 other NLU tasks.&lt;/p&gt;

&lt;h2 id=&quot;introducing-lm-bff&quot;&gt;Introducing LM-BFF&lt;/h2&gt;

&lt;p&gt;Finally, let me introduce our ACL‚Äô21 paper, ‚Äú&lt;a href=&quot;https://arxiv.org/abs/2012.15723&quot;&gt;&lt;strong&gt;Making Pre-trained Language Models Better Few-shot Learners&lt;/strong&gt;&lt;/a&gt;‚Äù, abbreviated as LM-BFF (better few-shot fine-tuning of language models; alternatively, language models‚Äô best friends forever). LM-BFF is a suite of simple techniques combined for fine-tuning pre-trained LMs on only a small number of training examples, including&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Prompt-based fine-tuning&lt;/strong&gt;, along with a novel method for &lt;strong&gt;automatic prompt generation&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;A dynamic and selective method for incorporating &lt;strong&gt;demonstrations&lt;/strong&gt; in context.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We evaluate LM-BFF in a rigorous few-shot setting (as mentioned above) and show that LM-BFF can drastically outperform standard fine-tuning by up to 30% absolute improvement (on SNLI) and 11% on average. You can find our code at this &lt;a href=&quot;https://github.com/princeton-nlp/lm-bff&quot;&gt;github repo&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prompt-based-fine-tuning&quot;&gt;Prompt-based fine-tuning&lt;/h3&gt;

&lt;p&gt;I have already discussed what prompt-based fine-tuning is‚Äîformulating the task as a (masked) language modeling problem with &lt;em&gt;templates&lt;/em&gt; and setting the expected output for each class as &lt;em&gt;label words&lt;/em&gt;. We design manual templates and labels words as listed below.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-5.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Manual prompts (templates + label words) used in our experiments. S1 and S2 represent the input sentences.&lt;/p&gt;

&lt;p&gt;However, hand-crafting good prompts can be tricky, requiring domain expertise, and the outcome can be unintuitive. In the following table, we show how sensitive few-shot models are to the small perturbations in the prompt.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-6.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;50%&quot; /&gt;&lt;br /&gt;The impact of different templates and label words. We take RoBERTa-large (Liu et al., 2019) and 16 train/dev examples for each class.&lt;/p&gt;

&lt;p&gt;We observe that if the template is fixed, the better the label words match the ‚Äúsemantic classes‚Äù, the better the result is. For example, for SST-2, great/terrible &amp;gt; good/bad &amp;gt; cat/dog &amp;gt; dot/cat &amp;gt; terrible/good (although it‚Äôs unclear why RoBERTa thinks üê± is more positive than üê∂). From SNLI, we see that if we put [MASK] at the end or swap the two sentences, there could be a &amp;gt;10% performance drop. This urges us to find a better way beyond manual prompts‚Äîautomatic prompt search.&lt;/p&gt;

&lt;h3 id=&quot;automatic-prompt-search&quot;&gt;Automatic prompt search&lt;/h3&gt;

&lt;p&gt;We separate the automatic prompt search into two parts‚Äîautomatically searching label words and searching templates.&lt;/p&gt;

&lt;p&gt;For automatic label word search, our goal is to find a set of label words that can maximize the dev performance, &lt;em&gt;given a manual template&lt;/em&gt;. A naive way to do so is to brute-force search all combinations of words. However, it does not work since the search space is exponential in the number of classes, and the method is prone to uncover spurious correlations and overfit. Instead, we first construct a candidate word set $V^c$ for each class $c$: denote $D_{train}^c$ as all the training examples with class $c$, we find the top-k words that maximize the LM probability at [MASK] given the template and $D_{train}^c$. Then we enumerate all word combinations of $V_c$ and find the top-n combinations that maximize zero-shot accuracy on the training set. In the end, we fine-tune all the n combinations and rerank them by the dev performance. We find that both brute-force search in the pruned space and the fine-tuning reranking are quite helpful regarding the final performance.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-7.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;70%&quot; /&gt;&lt;br /&gt;Our approach for template generation.&lt;/p&gt;

&lt;p&gt;For automatic template search, the goal is similar: find the template that maximizes the dev accuracy, &lt;em&gt;given the manual label words&lt;/em&gt;. We use T5 to generate many template candidates in an &lt;em&gt;out-of-the-box manner&lt;/em&gt;, and then rerank them by fine-tuning and dev performance. T5 is a seq-to-seq model and is pre-trained with a fill-in-the-blank objective, making it perfect for generating the template. Take sentiment classification (figure above) as an example, we concatenate the input example and the corresponding label word, and insert &lt;X&gt; and &lt;Y&gt; (mask tokens for T5) around the label word. Note that we want the T5 model to generate conditioned on all the few-shot training examples, so at each position, we take the sum of the log likelihood of all instances (you can refer to our paper for details). We use beam search with a large width (100) to get a large number of high-quality templates in the end.&lt;/Y&gt;&lt;/X&gt;&lt;/p&gt;

&lt;p&gt;The table below shows some examples generated by our automatic prompt search. You can see that for automatic template search, most templates fit the context and the manual label words well, although there are some potentially biased ones (e.g., ‚Äúno‚Äù in SNLI templates). Although mostly looking intuitive, the label-word results do contain some mysterious abnormalities (e.g., ‚ÄúHi‚Äù for the entailment class in SNLI).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-8.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Automatic prompt search results.&lt;/p&gt;

&lt;h3 id=&quot;incorporate-demonstrations&quot;&gt;Incorporate demonstrations&lt;/h3&gt;

&lt;p&gt;I have introduced how GPT-3 uses demonstrations in context: randomly sampling examples from the training set and concatenating them in arbitrary order, which is problematic in many aspects: the input lengths of pre-trained LMs are limited, especially for smaller ones (usually 512); it is hard to grasp meaningful patterns if the examples are concatenated in random orders; demonstrations that look far dissimilar to the input instance may be unhelpful or even cause confusion. Thus we propose this dynamic and selective way of incorporating demonstrations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;During both training and inference, we randomly sample one example for each class from the training set and concatenate them (the first figure in this post gives an example). For inference, we sample multiple sets of demonstrations and ensemble the results in the end.&lt;/li&gt;
  &lt;li&gt;We only sample demonstrations that are closely related to the input. For example, if the input is a movie review, then sampling a movie review is much more helpful than a restaurant review. We take SBERT (&lt;a href=&quot;https://arxiv.org/abs/1908.10084&quot;&gt;Reimers and Gurevych, 2019&lt;/a&gt;) to encode the sentences, calculate the cosine similarities between the input and all the examples from the training set, and only sample from the top 50% examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-9.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;Our main results (RoBERTa-large; each class has 16 training examples; results (standard deviation) are averaged over five splits). &quot;GPT-3&quot; in-context learning: use demonstrations in the GPT-3 style, but still take the fixed RoBERTa-large model. FT: fine-tuning; man: manual; auto: automatic templates.&lt;/p&gt;

&lt;p&gt;The above table shows our main experimental results. Here are the main takeaways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using prompts is great, even in the zero-shot case. Also, GPT-3 style learning does not constantly improve the results over the zero-shot model, suggesting that fine-tuning is still needed.&lt;/li&gt;
  &lt;li&gt;Prompt-based fine-tuning is much better than standard fine-tuning, with either manual or automatic prompts. On many tasks, automatic templates can get better results than manual ones.&lt;/li&gt;
  &lt;li&gt;Incorporating demonstrations can further bring significant improvement, showing that even with fine-tuning, adding demonstrations in context can help with the few-shot tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also have many interesting experiments in the paper, showing how automatic prompt generation can be combined with ensemble, and how different demonstration and automatic prompt policies affect the performance. In the end, we show that how the comparison between standard fine-tuning and LM-BFF goes with different numbers of training examples. As it clearly demonstrates, LM-BFF almost saturates its performance on simple tasks like SST-2 with only 32 training examples, and on harder tasks like SNLI, it brings a clear advantage over fine-tuning consistently, until the two converge with nearly one thousand training examples.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/2022-12-15-prompting/image-10.png&quot; alt=&quot;Responsive image&quot; class=&quot;figure-img img-fluid&quot; style=&quot;margin:20px&quot; width=&quot;70%&quot; /&gt;&lt;br /&gt;Standard fine-tuning vs LM-BFF with increasing K (# training examples per class).
&lt;/p&gt;

&lt;p&gt;There are, of course, limitations to our method. There is still large space for improvement in accuracy, and just like standard fine-tuning, LM-BFF is heavily affected by the large variance in few-shot training. Though the automatic prompt achieves on par or better performance than manual ones, it still requires some manual design (auto templates start from manual label words, and auto label words start from manual templates). Finally, prompt-based fine-tuning itself favors certain tasks that (1) can be posed as a ‚Äúfill-in-the-blank‚Äù problem, (2) have relatively short inputs, and (3) do not contain many output classes. These are all open questions left for future work.&lt;/p&gt;

&lt;p&gt;The paper was released at the end of 2020, and there have been lots of exciting advances about few-shot or prompting since then. Nevertheless, LM-BFF is unique in its study in automatic prompt generation and incorporating demonstrations in fine-tuning. Compared to recent soft-prompt approaches, LM-BFF (and other natural-language-prompt-based methods) has a vast advantage in smaller language models and few-shot scenarios. We hope that our work can inspire further exploration in this direction.&lt;/p&gt;

&lt;p&gt;To conclude, in this post I discussed a lot of recent progress about natural-language prompts, soft prompts, and in-context learning, and introduced our LM-BFF paper. I believe prompting is a very promising direction in the years to come. In a broader context, prompt-based methods are about how to better mine the knowledge (about facts, reasoning, understanding sentiment, etc.) from self-supervised learning (pre-training), and efforts in this direction can help squeeze the potentials of LMs and make them better and better learners to our world.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks Danqi Chen and Adam Fisch for proofreading the article and their helpful comments!&lt;/p&gt;</content><author><name></name></author><summary type="html">Starting from BERT (Devlin et al., 2019), fine-tuning pre-trained language models (LMs) with task-specific heads on downstream applications has become standard practice in NLP. However, the GPT-3 model with 175B parameters (Brown et al., 2020) has brought a new way of using LMs for downstream tasks: as the title ‚ÄúLanguage Models are Few-Shot Learners‚Äù suggests, GPT-3 can well handle a wide range of tasks with only a few examples by leveraging natural-language prompts and task demonstrations as context, while not updating the parameters in the underlying model. The giant model size of GPT-3 is an important factor for its success, while the concept of prompts and demonstrations also gives us new insights about how we can better use language models.</summary></entry><entry><title type="html">Forbidden City</title><link href="http://localhost:4000/blog/2019/12/20/forbidden_city/" rel="alternate" type="text/html" title="Forbidden City" /><published>2019-12-20T00:00:00-05:00</published><updated>2019-12-20T00:00:00-05:00</updated><id>http://localhost:4000/blog/2019/12/20/forbidden_city</id><content type="html" xml:base="http://localhost:4000/blog/2019/12/20/forbidden_city/">&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01270.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01166.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01107.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;figure&quot;&gt;
    &lt;img src=&quot;/assets/forbidden_city/DSC01106.jpeg&quot; style=&quot;width: 100%&quot; /&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>