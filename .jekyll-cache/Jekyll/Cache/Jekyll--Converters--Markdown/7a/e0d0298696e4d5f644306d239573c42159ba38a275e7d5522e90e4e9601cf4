I"p<p>Starting from BERT (<a href="https://arxiv.org/abs/1810.04805">Devlin et al., 2019</a>), fine-tuning pre-trained language models (LMs) with task-specific heads on downstream applications has become standard practice in NLP. However, the GPT-3 model with 175B parameters (<a href="https://arxiv.org/abs/2005.14165">Brown et al., 2020</a>) has brought a new way of using LMs for downstream tasks: as the title “Language Models are Few-Shot Learners” suggests, GPT-3 can well handle a wide range of tasks with only a few examples by leveraging natural-language <em>prompts</em> and task <em>demonstrations</em> as context, while not updating the parameters in the underlying model. The giant model size of GPT-3 is an important factor for its success, while the concept of prompts and demonstrations also gives us new insights about how we can better use language models.</p>
:ET